title,url,image,category,date,content
NASA's Green Fuel Satellite Jumps Its First HurdleHailey Lynne McKeefry,https://www.eetimes.com/document.asp?doc_id=1334905,https://m.eet.com/media/1312663/Ball_Aerospace_commissions_small_satellite.jpg,Military & Aerospace Designline,07.10.19,"Like the auto industry, space flight is going green by looking for new and sustainable fuel sources. Today, it is eying fuel options that are safer to handle than the highly toxic fuel, hydrazine. Last week, Bell Aerospace officially commissioned NASA's Green Propellant Infusion Mission (GPIM) and begun on-orbit testing of a non-toxic, high-performance propellant. The mission launched on June 25, 2019 at 2:30 a.m. EDT on board a SpaceX Falcon Heavy rocket. ""This mission has been an excellent example of an industry-led team involving multiple NASA centers, the Air Force and industry partners to test this new high-performance fuel using a Ball small satellite,"" said Dr. Makenzie Lystrup, vice president and general manager, Civil Space, at Ball Aerospace.The new green propellant will be an enabling technology for commercial spaceports operating across the United States, according to NASA.The green propellant makes fuel loading safer, cheaper, and more efficient.  and much less costly. The mission hopes to shorten ground processing time from weeks to days, which in turn reduces costs, and to simplify building and operating satellites. “The space technology infusion mission also strives to optimize performance in new hardware, system and power solutions while ensuring the best value for investment and the safest space missions possible,” NASA claimed.Ball Aerospace designed and built the Ball Configurable Platform (BCP) small satellite, which contains NASA's first opportunity to demonstrate the practical capabilities of a ""green"" propellant and propulsion system in orbit. The GPIM project will demonstrate the practical capabilities of this Hydroxyl Ammonium Nitrate fuel/oxidizer blend, which reduces toxicity levels compared to hydrazine and is easier and safer to store and handle.  The alternative to conventional chemical propulsion systems is called AF-M315E and was developed by the Air Force Research Laboratory.  “AF-M315E is expected to improve overall vehicle performance,” NASA said. “It boasts a higher density than hydrazine, meaning more of it can be stored in containers of the same volume. In addition, it delivers a higher specific impulse, or thrust delivered per given quantity of fuel, and has a lower freezing point, requiring less spacecraft power to maintain its temperature.” GPIM is part of NASA's Technology Demonstration Missions program within the Space Technology Mission Directorate (STMD). Ball Aerospace’s Christopher McLean serves as the principal investigator for the program. Meanwhile, thrusters designed and built by Aerojet Rocketdyne provide propulsion for the spacecraft.""The successful commissioning of our thrusters and propulsion system is a positive step toward fully qualifying our green propulsion system in space,"" said Joe Cassady, executive director of space at Aerojet Rocketdyne. ""This technology will enable propulsive capabilities for a new generation of small satellites, including new mission capabilities.""The program will test the thruster capabilities by verifying the propulsion subsystem, propellant performance, thruster performance, and spacecraft attitude control performance over the first three months. Testing of secondary payloads will occur over the remaining time in the 13-month testing period. Ball Aerospace, as the prime contractor, is tasked with system engineering; flight thruster performance verification; ground and flight data review; spacecraft bus; assembly, integration and test; and launch and flight support.  At the end, the project will present the fuel, as well as compatible tanks, valves and thrusters, for use by NASA and for commercial spaceflight applications. The small satellite, which is about the size of a mini refrigerator, was built in just 46 days.  Currently, two other BCP small satellites are also on orbit: STPSat-2, which launched in November 2010, and STPSat-3, which launched in November 2013. The two STP satellites were built for the U.S. Air Force Space Test Program's Standard Interface Vehicle (STP-SIV) project, Ball said.— Hailey Lynne McKeefry, Editor in Chief,  EBN "
Eta's Ultra Low-Power Machine Learning Platform Maurizio Di Paolo Emilio,https://www.eetimes.com/document.asp?doc_id=1334903,https://m.eet.com/media/1312661/Etafig4.jpg,AI & Big Data Designline,07.10.19,"Eta Compute has developed a high-efficiency ASIC and new artificial intelligence (AI) software based on neural networks to solve the problems of edge and mobile devices without the use of cloud resources.Future mobile devices, which are constantly active in the IoT ecosystem, require a disruptive solution that offers processing power to enable machine intelligence with low power consumption for applications such as speech recognition and imaging.These are the types of applications for which Eta Compute designed its ECM3531.The IC is based on the ARM Cortex-M3 and NXP Coolflux DSP processors. It uses a tightly integrated DSP processor and a microcontroller architecture for a significant reduction in power for the intelligence of embedded machines. The SoC includes an analog to digital converter (ADC) sensor interface and highly efficient PMIC circuits. The chip also includes I2C, I2S, GPIOs, RTC, PWM, POR, BOD, SRAM and Flash. The patented hardware architecture (DIAL) is combined with fully customizable CNN-based algorithms to perform machine learning inference in hundreds of microwatts.The processor, named Tensai, can be used with the popular TensorFlow or Caffe Software. This solution can support a wide range of applications in audio, video and signal processing where power is a strict constraint, such as in UAV (unmanned aerial vehicles) markets, in the Internet of things (IoT) and wearable markets.ECM3531SP includes pretrained learning machine speech recognition and keyword spotting applications. ECM3531PG pretrained photoplethysmogram (PPG) application and ECM3531SF includes machine algorithms for fusion of gyro, magnometer, and accelerometer sensors (Figure 1).The patented hardware architecture is combined with the fully customizable Eta Compute algorithms based on CNN, LSTM, GRU and SNN (spiking neural network) to perform machine learning inference in very few mW. Eta provides kernel software for convolutional neural networks on Coolflux's DSP, which are scalable compared to other NN (neural networks) and which will reduce an additional 30% of the power with asynchronous technology.Tensai's computational properties offer 30-fold power reduction in a specific CNN-based image classification benchmark — unlike other Cortex-M7-class microcontrollers. Eta Compute has reached 0.04mJ per image out of 8 million operations (figure 2).The high energy efficiency ASIC and the CNN software developed by Eta Compute avoid the need for numerous training samples for peripheral applications, where the amount of resources (both memory and calculation) is limited. A recent benchmark reached by Eta Compute was an improvement of 2-3 orders of magnitude in the efficiency of the model compared to various variants of neural networks for keyword recognition by consuming only 2 mW of power.For sensing applications, particularly for motion and environmental sensors, the Eta Compute methodology allows sensor hubs to execute more extensive sensor algorithms by providing data and updates in real time from mobile network devices and the Internet of Things (IoT). The collaboration with Rohm Semiconductor has enabled the development of a Wireless Smart Ubiquitous Network (Wi-SUN) which is compatible with sensor nodes. The nodes will combine Rohm sensor technology and Eta Compute's low-power MCUs to offer solutions for intelligent utility and IoT networks for smart cities. They will be designed for frequent low-latency communications that absorb less than 1 μA during rest and, more importantly, only 1 mA during detection.Eta Compute believes that neural network technology will play a key role in enabling intelligent peripheral devices. Thanks to the ability to learn and process sensory data directly on the margins in an energy efficient manner, new ASICs will provide relief to the bandwidth requirements needed to send raw data to a cloud-based learning service. The energy efficiency of neuromorphic processors will also allow ""always on"" solutions without suffering from handicaps deriving from power requirements.Next page: Where Tensai fits into the AI marketNext page: Where Tensai fits into the AI market  "
Fab Tool Outlook Clouds Further Dylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334906,https://m.eet.com/media/1312666/semi_west-2019_800_2-min.png,Memory Designline,07.10.19,"SAN FRANCISCO — The semiconductor industry’s temperature has gone from lukewarm to freezing as the year has evolved, with analysts who had once forecast modest growth now projecting that IC sales will contract by a double-digit percentage.This week, at Semicon West — the semiconductor equipment and materials industry’s annual tradeshow — the SEMI trade association delivered revised forecasts for the year that call for wafer fab equipment sales to decline by roughly 18% this year. SEMI now expects sales of semiconductor materials to decline slightly this year.RecommendedIntel Shows Next Steps in PackagingRecommendedIntel Shows Next Steps in Packaging“After three years of consecutive growth from 2016 through 2018, we are seeing a big correction in 2019,” said Clark Tseng, director of industry research and statistics at SEMI. “Actually, we saw this coming in the second half of 2018, but the correction seems to be bigger than we expected.”Indeed, after consecutive all-time record sales of $56.6 billion in 2017 and $64.5 billion in 2018, wafer fab equipment sales are now projected to fall to $52.7 billion in 2019. And while SEMI is forecasting that equipment sales will rebound to grow 11% next year, reaching $58.8 billion, whether that will actually happen is a matter of considerable debate.A number of factors that dragged down chip sales in the first half of the year are still at issue, including the near-freefall of memory chip prices from their lofty peak in early 2018, excess inventory at many suppliers, weaker-than-expected demand, and ongoing trade tension between the U.S. and China. Tseng said that he expects these headwinds to continue in the second half of the year and that slower-than-expected inventory digestion could result in chip inventories remaining out of whack through the end of 2019.“The demand has not fallen off, but the inventory issue was a lot more severe than people expected,” said Mario Morales, vice president of enabling technologies, storage, and semiconductor research at IDC.Amid oversupply, memory chip suppliers have slashed capital expenditures for the year by about 40% compared with 2018. Tseng said that he expects a recovery in capex for both DRAM and NAND flash to recover in 2020, beginning with NAND. He expects DRAM capex to recovery to roughly the level of 2017 next year and expects to see NAND capex increase by more than 50% compared to this year.But not all analysts are convinced that 2020 will see a recovery in capital equipment spending.“I’m looking for an equipment recovery starting in the Q3 2020 time frame and not before that,” said CJ Muse, a managing director and senior equity research analyst at investment banking firm Evercore ISI, during the annual Semicon West “Bulls and Bears” panel."
Memory Startup Targets High-Performance ComputingNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334908,https://m.eet.com/media/1312673/ComputerrunningBlueshiftFPGAcard.jpg,Memory Designline,07.10.19,"A Cambridge UK-based startup is looking to address the memory bottleneck (or tailback) in high-performance computing with a new memory chip design dedicated to handling large data sets and time-critical data.Blueshift Memory, which was started by and currently consists of a team of three computer scientists, has successfully demonstrated its new memory model in a Xilinx FPGA. The company is now on the hunt for investors to fund the development of a chip.We spoke to Peter Marosan, CTO of Blueshift Memory, to find out exactly what the company is trying to do. It is essentially optimizing the memory architecture so that large data sets and time-critical data can be more efficiently handled, hence speeding up memory access speeds up to 1,000 times for specific data-focused applications.He said, “We use DRAM as the main memory (not Flash) and store the data in a different way inside the memory by changing the wiring in the memory module. Current memory modules are too general; our solution is targeted at time-critical data and large data sets.”Marosan said the new memory design is one in a wave of radical changes in computer memory emerging from different companies around the world, which aim to address the struggle for high performance computers to keep pace with society’s spiraling data demands. He said computer memory chips (usually RAM) are just not improving as quickly as their central processing units (CPUs).This creates a ‘tailback’ in processes where high-performance computers perform large-scale operations, like database searches with millions of possible outcomes. Data stacks up in a slow-moving queue between the CPU and the less-efficient memory. Blueshift said its new design reorganizes the way in which a memory chip handles these operations, so that it delivers data to the CPU much faster. With it, operations could take minutes, or even seconds, rather than hours.Blueshift’s team analyzed and categorized thousands of algorithms used by companies to solve complex data problems. They then designed the chip so that it arranges data in preparation for these tasks — an approach that could be combined with any type of memory cell technology.Its initial model implemented in an FPGA to emulate the chip’s effects has yielded some impressive results, according to Blueshift. Simulations using this card suggest that the chip could, for example, make searches of vast databases used to match fragments of DNA in scientific research, or in criminal investigations, 100 times faster. In other tests, algorithms used in weather forecasting and climate change modelling could also run 100 times faster using the chip. Better memory chips could also accelerate the more data-hungry aspects of home computing. Blueshift’s prototype makes rendering films in video editing software 10 times faster, for example; it could also improve the processing speeds of virtual reality headsets by a factor of up to 1,000.The company said its design could also make it much easier to program some data operations, because it would remove the need to include complex instructions about how to handle the vast quantities of data involved. “It would make some big data programming as straightforward as the basic data searches that computing students learn to write in high school,” Marosan said.For example, the artificial intelligence (AI) in autonomous vehicles needs to process huge quantities of data quickly to make decisions. Or in connected in smart cities, fast, real-time data processing on a large scale will be essential to manage traffic flows, utility supplies, and even evacuation procedures in times of danger.Once the company manages to validate its own chip, it then plans to license its technology to memory module manufacturers. Marosan said, “Our strategy is ready, our prototype is ready, and we know who our partners will be. We now just need the funding to create the chip.”Marosan told us they are having conversations currently with DRAM and FRAM manufacturers, and also looking to speak with high bandwidth memory (HBM) makers."
Intel Shows Next Steps in PackagingRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334899,https://m.eet.com/media/1312645/IntelCoEMIBcover-min.png,SoC Designline,07.09.19,"SAN FRANCISCO — Intel gave a first glimpse of three packaging technologies on its roadmap at a gathering on the sidelines of Semicon West here. The most interesting of the three may debut in the exascale supercomputer Intel is building for the U.S. Department of Energy.The trio of techniques aim to give Intel’s processors an edge at a time when advances in conventional silicon scaling are slowing and getting more expensive. They arrive as rival TSMC expands its portfolio of chip stacks, and two consortia hope to set standards in the area.MDIO is the next-generation of Intel’s AIB, a physical interface for stacking chiplets it released last year as part of a DARPA program. Intel claims MDIO is on par with advances rival TSMC announced last month. It will use the interface in chip stacks starting sometime in 2020 but has not decided if it will make the spec open.The most interesting of the three new techniques is Co-EMIB. A combination of Intel’s latest 2D and 3D stacking techniques, it likely will see its first use as a way to link CPU and GPU cores in the Aurora supercomputer Intel and Cray won a $500 million contract to deliver before the end of 2021.Prototypes shown here of Co-EMIB wafers and devices stacked 18 small die on one large one using the Foveros 3D technique Intel announced in December. Two of the devices were then connected using four of its Embedded Multi-die Interconnect Bridge (EMIB) links using 45- and 55-mm bump pitches.Intel has shipped as many as a million devices using EMIB in Stratix X FPGAs and Kaby Lake G, an integrated CPU/GPU module. Next year it will ship Lakefield, an integrated notebook processor slated to be its first chip using Foveros.Currently,Intel suffers transport delays making the face-to-face Foveros stacks because the process is split between a front-end line in Oregon and a back-end line in Arizona. Once it moves the process to one location, turn around times should be about two weeks.The third new option is so far just a research project. Omni Directional Interconnect (ODI) is a 70mm thick vertical link for delivering power to a chip.Next page: Packages with 2X reticle dice on the horizonNext page: Packages with 2X reticle dice on the horizon"
"A Week in European Automotive: Reinventing the Car, Last Mile RobocarsNitin Dahad",https://www.eetimes.com/document.asp?doc_id=1334900,https://m.eet.com/media/1312650/REEFlatChassis.png,Automotive Designline,07.09.19,"“We’re disrupting the automotive industry!"" That’s a common claim from many briefings we get from emerging startups. Last week was not unusual — one told us how they’ve solved the challenge of producing roadworthy autonomous delivery vehicles for last mile delivery; another has ‘reimagined’ the vehicle by integrating all the components found under the hood of a car into the wheel.Both are at early stages; both claim investment interest from retailers and Tier 1s in the automotive manufacturing supply chain.A Modular EV Platform, All Built into The WheelA Modular EV Platform, All Built into The WheelIsraeli startup REE, which came out of stealth mode today, has a flat and modular platform that fundamentally changes the way electric vehicles (EVs) will be built. The company integrates all of the components formerly found under the hood into the wheel. It says it offers optimal freedom of design, and the potential for multiple body configurations on a single platform.Putting the motor, steering, suspension, drivetrain, sensing, brakes, thermal systems and electronics in the wheel leaves a ‘flat’ platform. This provides a low center of gravity to maximize efficiency and supports the vehicle’s agility and stability. The design also drastically reduces its footprint, weight, and improves both energy-efficiency and performance — aspects crucial for electric and autonomous vehicle development.In an interview with EE Times, Daniel Barel, co-founder and CEO of REE, told us, “The industry is still using 80-year old concepts for car design, even though the technology has changed. There have been many advances, but these have still only been incremental, and it doesn’t make sense to build EVs based on these old concepts.”He said they wondered, “What if we could build a rolling chassis platform that is modular and would still be appropriate even one hundred years from now?” He added that the single biggest expenditures for an OEM was the platform, which can cost billions of dollars, which is why they started developing a platform that could support any shape and size of car.REE’s platform provides automakers, mobility providers and delivery companies a tailor-made solution. Based on a novel quad-motor system, and including active height-levelling suspension, steer-by-wire and a smart quad-gear box, the technology provides the basis of any type of vehicle from a high performance car able to do 0-60 mph in less than 3 seconds to an off-road SUV with advanced active suspension technology. The platform can also be used as the base of a robotaxi or even a 10-ton cross country truck.By adopting a universal framework, this can replace the multiple platforms that OEMs develop, resulting in substantial savings. The design and validation of each platform traditionally costs manufacturers $20 billion. By enabling them to utilize one platform for all of their vehicles, costs could be slashed, while performance, safety, comfort and energy efficiency can all be matched or improved.Barel said they’ve been validating the technology for the last six years in stealth. In early 2018, they established some initial discussions with a Tier 1 and an OEM to gauge interest. “They loved our idea and became investors and also wanted us to expedite what we were doing.” Since then, REE has been building its network and establishing relationships with players such as Mitsubishi, NSK, Tenneco and FCA. The company is essentially the system integrator, developing both the hardware, including electronics, and software, selling its wheel to the tier one manufacturers and OEMs.Although Barel wasn’t prepared to disclose funding to date, he said it’s sub-$100 million. The company employs around 100 people, 50 of whom are based in Israel. He added that the full solution will be presented with its partners later this year and manufactured globally. Barel is also co-founder of SoftWheel, a developer of in-wheel suspension technology for personal mobility, which has raised $40 million to date and announced investment from Mitsubishi Corp. last year.Next page: ‘First’ Roadworthy Last Mile Autonomous Delivery Vehicle Next page: ‘First’ Roadworthy Last Mile Autonomous Delivery Vehicle "
Cisco Bids $2.6B for AcaciaRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334901,https://m.eet.com/media/1312653/fiber-optic-2749588_1280-min.jpg,Wireless and Networking Designline,07.09.19,"SAN JOSE, Calif. — Cisco aims to increase both its revenues and profit margins in an optical networking market shifting to components with its $2.6 billion bid for Acacia Communications. The deal would put the Ethernet giant in the long-distance market for the first time and help it retain business from web giants who are increasingly buying optical modules directly from component makers.Acacia sells coherent optics, digital signal processors (DSPs), photonic integrated circuit modules, and transceivers running up to 600G for large data centers and telcos, as well as OEMs such as Cisco rival Arista Networks. Its latest-generation modules can be used for distances spanning everything from inside a data center to undersea cables.“That helps simplify things for customers who won’t have to think about purpose-built components for those [different distance] segments now,” said Bill Gartner, general manager of Cisco’s optical system and component group.Cisco has an existing business in optical modules that it provides for its own systems and sells to other OEMs. However, it is currently “primarily 10–100G — all short-reach — for inside the data center … so this is a top-line expansion,” said Gartner, who insisted that Cisco will be even-handed as a module supplier.“If we are going to make this successful, we have to provide third parties what they want, when they want it, and at the right price point. We can’t make this successful if we give Cisco preference on any of those parameters.”The reason that the largest data center operators are increasingly buying optical modules directly from component suppliers is to avoid the high margins charged by switch and router makers. The Acacia deal would help Cisco retain that business and give it an edge in lower module costs than its OEM rivals.Cisco pegs coherent optical networks as a multi-billion-dollar market, shifting from systems to modules, but still in an early stage. As the industry shifts “from chassis to pluggable [modules], we need to think about how the network architecture will evolve, and we want to be in a position to influence that architecture with customers,” said Gartner.Cisco is already shipping optical systems supporting 400G links. However, the optical modules to enable them won’t sample until late this year and won’t ramp to volume sales until early 2020, he said.The deal will make it harder for Cisco’s OEM rivals — Arista, Ciena, Huawei, Infinera, Juniper, and Nokia — to compete in core routers, Ethernet switches, and optical transport equipment, said Vladimir Kozlov, chief executive of market watcher LightCounting. However, the rivals may throw more business to module makers such as Finisar, Fujitsu, Lumentum, NEC, and others.Longer term, Acacia’s expertise combined with talent acquired earlier with Luxtera — a supplier of short-reach optical modules used inside the data center — will help Cisco manage a transition to silicon photonics.“We will see a silicon/optics merger,” said Kozlov. “The industry is still thinking through what that architecture looks like, what’s in silicon, and what comes off the chip. There’s a lot of heavy lifting to figure out, but with Acacia, Cisco, and Luxtera, we have all the parts to drive whichever way the industry wants to go.”Cisco currently has more than 2,000 optical networking customers, a mix of web giants, telcos, and large businesses. About a quarter of Acacia’s sales are DSPs and photonic integrated circuits, and the rest are modules.Cisco declined to discuss any plans for layoffs or cost savings until the deal closes in the second half of its fiscal 2020. The deal requires regulatory approvals, including one from China, a potential snare given the U.S./China trade war in tech.The U.S. ban last year on sales to ZTE — Acacia’s largest customer — and its ban on Huawei this year limited Acacia’s growth and lowered its valuation, LightCounting said. Nevertheless, it called the $2.6 billion bid with its 40% premium on Acacia’s stock price a “respectful value” for the 400-person company."
On-Chip Cooling Emerges to Take the HeatGeorge Leopold,https://www.eetimes.com/document.asp?doc_id=1334897,https://m.eet.com/media/1312607/JetCool1.png,Military & Aerospace Designline,07.08.19,"Processing-intensive applications ranging from AI chips and hyperscale datacenters to aerospace applications and all those devices being integrated into electric cars are generating boat-loads of heat. As conventional thermal management techniques fail to keep pace up with all that hot air, an MIT spinoff has come up with a new way to cool electronics.After nearly five years of development, JetCool Technologies recently emerged from stealth mode with an approach it calls micro-convective cooling. The technology uses small fluid jets the company’s CEO said can be integrated into electronic devices.JetCool CEO Bernie Malouin said the ability to place the fluid jets adjacent to where heat is being generated would yield a ten-fold increase in cooling efficiency compared with the current state of the art. The startup also claims comparable size and weight reductions for aerospace applications.Micro-convective cooling also is touted as delivering 90 percent of the performance of current cooling technologies since it can be integrated as a heat sink on a silicon substrate without the need for exotic semiconductor materials or complex coding, Maloiun said.Spun out from MIT in January, JetCool seeks to leverage a $2 million investment in technology development to address growing cooling requirements as datacenters, for example, strain to handle machine learning and other computing-intensive workloads. Malouin also noted the need for better cooling technologies in electric vehicles as more devices are integrated into designs, straining power management systems.“Those folks are pushing the limits in terms of cooling,” Malouin noted in an interview.The startup rolled out its micro-convective cooling approach at this year’s International Microwave Symposium in Boston. JetCool was named the event’s “Next Top Startup.”JetCool is targeting its technology at military and aerospace applications such as power amplifiers and RF components. E-car applications include power inverters and infotainment consoles. It is also geared to wireless applications like optical networking and 5G transmitters.The best opportunity to scale the cooling technology appears to be enterprise datacenters that are increasingly powered by graphics processors — often coupled with CPUs — as well as emerging AI chips and ASICs. All are being employed to handle demanding workloads as companies roll out more distributed applications.Many of those use cases represent a doubling or tripling of power density levels, prompting datacenter operators to look for new ways to dissipate heat generated by racks of servers.The startup’s approach also reflects expanding research into new ways to cool electronics as heat generation and power dissipation soar. Current thermal management approaches involving remote cooling only work by adding weight and volume to electronic components.MIT launched its micro-convective cooling effort in 2012. The same year, the Defense Advanced Research Projects Agency initiated a related program called Intrachip/Interchip Enhanced Cooling. Also known as ICECool, the DARPA program explored embedded thermal management techniques, including the integration of microfluidic cooling inside a substrate, chip or package. Among the goals was incorporating new thermal management techniques into the chip design process. — George Leopold is the former executive editor of EE Times and the author of Calculated Risk: The Supersonic Life and Times of Gus Grissom (Purdue University Press, Updated, 2018).— George Leopold is the former executive editor of EE Times and the author of  (Purdue University Press, Updated, 2018)."
"Xilinx Ships Heterogeneous Chips for AI, 5GSally Ward-Foxton",https://www.eetimes.com/document.asp?doc_id=1334898,https://m.eet.com/media/1312612/CAROXilinxVersalBlockDiagram.jpg,Programmable Logic Designline,07.08.19,"LONDON — Xilinx has shipped the first Versal devices to select customers as part of its early access program, a milestone for the company’s heterogeneous compute architecture. Versal devices use Xilinx’s adaptive compute acceleration platform (ACAP), part of the company’s strategy for modern workloads including high speed networking, 5G, and artificial intelligence (AI). Xilinx has shipped its first Versal ACAP device (Source: Xilinx)  Xilinx has shipped its first Versal ACAP device (Source: Xilinx) “Having our first Versal ACAP silicon back from TSMC ahead of schedule and shipping to early access customers is a historic milestone and engineering accomplishment,” said Victor Peng, president and CEO of Xilinx, in a statement. “It is the culmination of many years of software and hardware investments and everything we’ve learned about architectures over the past 35 years.”Built on TSMC’s 7-nm FinFET process technology, the first devices to ship are from the Versal Prime series (for a variety of applications) and the Versal AI Core series (for acceleration of AI inference workloads). According to Xilinx, the AI Core series can outperform GPUs by 8X on AI inference (based on sub-2ms latency convolutional neural network performance versus Nvidia V100).In an interview with EETimes Europe, Xilinx’s Nick Ni, director of product marketing for AI, software and ecosystem, said that in the AI accelerator market in particular, there is a lot at stake.“Everybody is betting on something, including [established players] and ASIC startups … we think this is an intelligent bet,” said Ni. “The truth is, nobody has captured the market in AI … it’s just really getting started. Everyone agrees the hardware will be the bottleneck of mass deployment — whoever gets the hardware right for this moving target workload, which is very hard to design for, [will win the bet].” Heterogeneous Compute  Ni said that the rapid pace of innovation in fields such as neural networks and artificial intelligence have so far left hardware running to catch up. Heterogeneous Compute “Developing an ASIC for this market might take a couple of years to develop, verify, and go to market — by then the workload has completely changed,” he said. “When workloads continue to change, efficiency goes down, or you can’t [continue to] support the feature sets needed. … With AI workloads, which require low power and high efficiency, you need domain specific architecture, that is, you need a specific accelerator for each problem that you have.”AI workloads are notoriously diverse and fast-moving in terms of structure. While all neural networks require huge amounts of compute and a lot of data transfer between different multiply-accumulate units (MACs), even basic image recognition workloads differ vastly depending on which neural network is used.“FPGAs have always been in the sweet spot of being able to work on all sorts of workloads efficiently,” he said. “AI has pushed us to the point where that is nice, but it’s not good enough: you really need ASIC-like performance in some portions of your workload, and you also need some flexibility to adapt to the latest innovations.”To accelerate neural networks efficiently in hardware, three things have to be customized for every AI network, Ni explained.First, the data path has to be customized. Data paths vary from the simplest feed-forward networks (e.g., AlexNet) to more complex paths with branches (e.g., GoogleNet), to the latest networks with skip connections and merging paths (e.g., DenseNet).Second, is precision. The less precision you use, the more power you save, which is important for inference on edge devices; AI workloads have different “sweet spots” in terms of the number of bits required.Third, how do you move data around the chip?“Many chip [makers] say they have huge peak performance, but if you can’t pump the data into the engine fast enough, most of the time it’s sitting idle,” said Ni. “The [pace of] innovation of AI is making this part difficult because the more complex the neural networks become, the more complex the data flow becomes, and how you move the data from memory to the engine and back efficiently becomes the bottleneck. This has to be customized for every network.” Domain Specific Architecture  Xilinx’s vision for neural network acceleration is a domain specific architecture in which the data path, precision, and memory hierarchy can all be customized. Domain Specific Architecture “We are pushing from general purpose architecture, from CPUs and GPUs, into special hardware architectures for each problem … creating a custom system for each problem,” he said. “That’s extremely difficult for GPUs and ASICs, but it’s easier for FPGAs as we can reprogram the different hardware architecture for each [neural network].” Xilinx ACAP platform combines programmable logic, CPU cores, DSP engines, and AI accelerator engines (Source: Xilinx)  Xilinx ACAP platform combines programmable logic, CPU cores, DSP engines, and AI accelerator engines (Source: Xilinx) Versal is the first device built on Xilinx’s ACAP. ACAP combines scalar processing blocks (a dual core ARM Cortex-A72 application processor and an ARM Cortex-R5F real-time processor) with adaptable hardware engines (its new name for programmable logic) and intelligent engines (specialized DSP and AI engines), plus memory and interfaces.The AI engine is “basically AI ASIC hardware — it has more flexibility to it [than an ASIC] but it achieves hundreds of TOPS for AI workloads,” Ni said. Up to 400 of these inference engines are included on the Versal AI Core devices.In between the blocks is a software programmable multi-terabit network-on-chip which enables data transfer between the engines, the memory, and the interfaces.“This has been one of the choking points for FPGAs in the past, routing between different logics could degrade the performance, and having an actual ASIC — ahardened network-on-chip topology — we can move the data at a very fast speed, like gigahertz kinds of speed,” said Ni.—  Software Programmability  While FPGAs traditionally had their own specialist programming languages, today’s AI accelerators will need to be accessible to both software and hardware developers and data scientists. Software Programmability “As a company, we have invested significantly in bringing up the ease of use of software tools and frameworks for AI and software,” said Ni, referring to Xilinx’s purchase of Chinese AI startup DeePhi last year.“If we want to go after AI and software developers, we have to support C, C++, OpenCV, OpenCL — those kinds of languages. Then for AI developers, new languages like Python, Caffe, TensorFlow. … We are transforming the way we are hiring more software [people] and investing into tools and frameworks.”Available solutions include Xilinx’s DNNDK and ML Suite platforms, which can compile deep learning frameworks for FPGA boards without writing any C code or RTL code, Ni said.“We will continue to enhance this in the future and import it to Versal,” he said.Versal currently comes with its own development environment with a software stack that includes drivers, middleware, libraries, and software framework support. The Versal AI Core and Versal Prime series will be available in the second half of 2019."
IoT Nets in Two-Horse LPWAN RaceRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334892,https://m.eet.com/media/1312576/LPWANIHScover-min.png,Internet of Things Designline,07.05.19,"SAN JOSE, Calif. — LoRa and cellular’s Narrowband-IoT (NB-IoT) are far ahead of a pack of low-power wide-area networks (LPWANs) staking out early design wins in the internet of things. The LTE-M version of 4G cellular is a distant third, and Sigfox trails, according to a new report from IHS Markit.The report suggests that a once wide-open field is beginning to narrow significantly. However, it’s still early days. IHS estimated that just 150 million LPWAN links were deployed in 2018, a figure that it expects to expand at a 63% compound annual growth rate to hit 1.7 billion links by 2023.It’s also worth noting that some alternatives are just emerging from the lab. For example, multiple vendors are shipping their first chips this year for a 900-MHz version of Wi-Fi called HaLow that’s expected to hold significant promise for long-range connections. And last year, research institute CEA-Leti announced early work on a new option based on a patented Turbo-FSK waveform.That said, IHS forecasts that NB-IoT and LoRa could claim 86% of all LPWAN links in 2023. “We think it is a two-horse race by 2023, with LoRaWAN more in private and NB-IoT mainly in public networks,” said Christian Kim, one of the authors of the report.Interestingly, Huawei’s HiSilicon division is the leading supplier of today’s NB-IoT chips, 90% of which are deployed in China. Taiwan’s Mediatek is second, and China’s RDA Unisoc is third in NB-IoT silicon. NB-IoT had its origins in technology from U.K. startup Neul, acquired by Huawei in 2014.The relatively sluggish market for LTE-M chips is led by Qualcomm, followed by Sequans and Altair. Semtech is by far the dominant supplier of LoRa chips, the current market leader among LPWANs.Overall, 54% of last year’s LPWAN deployments were in China versus about 23% each in the Americas and the EMEA region. Government-backed smart-city projects in China are driving NB-IoT today with deployments in smart meters, parking meters, and streetlights.“Most projects are using government money,” said Kim. “A lot of enterprises have not warmed to NB-IoT, even in China.”To some extent, the LPWANs are solutions seeking problems. IHS currently tracks 20 use cases proposed for the links.The China government is promoting use of NB-IoT in smart homes for electronic locks, smoke detectors, and other uses, and Huawei has talked about its use in agriculture for more efficient dairy farms. However, it has not yet taken hold in such applications to date, Kim said.In the U.S., Wi-Fi and Bluetooth by far dominate in the smart home. And in developed countries, dairy farms are already operating at high efficiency levels, he added.Next page: Sequans, IHS have different views of LTE-MNext page: Sequans, IHS have different views of LTE-M"
"Xilinx Ships Heterogeneous Chips for AI, 5GSally Ward-Foxton",https://www.eetimes.com/document.asp?doc_id=1334898,https://m.eet.com/media/1312612/CAROXilinxVersalBlockDiagram.jpg,Programmable Logic Designline,07.08.19,"LONDON — Xilinx has shipped the first Versal devices to select customers as part of its early access program, a milestone for the company’s heterogeneous compute architecture. Versal devices use Xilinx’s adaptive compute acceleration platform (ACAP), part of the company’s strategy for modern workloads including high speed networking, 5G, and artificial intelligence (AI). Xilinx has shipped its first Versal ACAP device (Source: Xilinx)  Xilinx has shipped its first Versal ACAP device (Source: Xilinx) “Having our first Versal ACAP silicon back from TSMC ahead of schedule and shipping to early access customers is a historic milestone and engineering accomplishment,” said Victor Peng, president and CEO of Xilinx, in a statement. “It is the culmination of many years of software and hardware investments and everything we’ve learned about architectures over the past 35 years.”Built on TSMC’s 7-nm FinFET process technology, the first devices to ship are from the Versal Prime series (for a variety of applications) and the Versal AI Core series (for acceleration of AI inference workloads). According to Xilinx, the AI Core series can outperform GPUs by 8X on AI inference (based on sub-2ms latency convolutional neural network performance versus Nvidia V100).In an interview with EETimes Europe, Xilinx’s Nick Ni, director of product marketing for AI, software and ecosystem, said that in the AI accelerator market in particular, there is a lot at stake.“Everybody is betting on something, including [established players] and ASIC startups … we think this is an intelligent bet,” said Ni. “The truth is, nobody has captured the market in AI … it’s just really getting started. Everyone agrees the hardware will be the bottleneck of mass deployment — whoever gets the hardware right for this moving target workload, which is very hard to design for, [will win the bet].” Heterogeneous Compute  Ni said that the rapid pace of innovation in fields such as neural networks and artificial intelligence have so far left hardware running to catch up. Heterogeneous Compute “Developing an ASIC for this market might take a couple of years to develop, verify, and go to market — by then the workload has completely changed,” he said. “When workloads continue to change, efficiency goes down, or you can’t [continue to] support the feature sets needed. … With AI workloads, which require low power and high efficiency, you need domain specific architecture, that is, you need a specific accelerator for each problem that you have.”AI workloads are notoriously diverse and fast-moving in terms of structure. While all neural networks require huge amounts of compute and a lot of data transfer between different multiply-accumulate units (MACs), even basic image recognition workloads differ vastly depending on which neural network is used.“FPGAs have always been in the sweet spot of being able to work on all sorts of workloads efficiently,” he said. “AI has pushed us to the point where that is nice, but it’s not good enough: you really need ASIC-like performance in some portions of your workload, and you also need some flexibility to adapt to the latest innovations.”To accelerate neural networks efficiently in hardware, three things have to be customized for every AI network, Ni explained.First, the data path has to be customized. Data paths vary from the simplest feed-forward networks (e.g., AlexNet) to more complex paths with branches (e.g., GoogleNet), to the latest networks with skip connections and merging paths (e.g., DenseNet).Second, is precision. The less precision you use, the more power you save, which is important for inference on edge devices; AI workloads have different “sweet spots” in terms of the number of bits required.Third, how do you move data around the chip?“Many chip [makers] say they have huge peak performance, but if you can’t pump the data into the engine fast enough, most of the time it’s sitting idle,” said Ni. “The [pace of] innovation of AI is making this part difficult because the more complex the neural networks become, the more complex the data flow becomes, and how you move the data from memory to the engine and back efficiently becomes the bottleneck. This has to be customized for every network.” Domain Specific Architecture  Xilinx’s vision for neural network acceleration is a domain specific architecture in which the data path, precision, and memory hierarchy can all be customized. Domain Specific Architecture “We are pushing from general purpose architecture, from CPUs and GPUs, into special hardware architectures for each problem … creating a custom system for each problem,” he said. “That’s extremely difficult for GPUs and ASICs, but it’s easier for FPGAs as we can reprogram the different hardware architecture for each [neural network].” Xilinx ACAP platform combines programmable logic, CPU cores, DSP engines, and AI accelerator engines (Source: Xilinx)  Xilinx ACAP platform combines programmable logic, CPU cores, DSP engines, and AI accelerator engines (Source: Xilinx) Versal is the first device built on Xilinx’s ACAP. ACAP combines scalar processing blocks (a dual core ARM Cortex-A72 application processor and an ARM Cortex-R5F real-time processor) with adaptable hardware engines (its new name for programmable logic) and intelligent engines (specialized DSP and AI engines), plus memory and interfaces.The AI engine is “basically AI ASIC hardware — it has more flexibility to it [than an ASIC] but it achieves hundreds of TOPS for AI workloads,” Ni said. Up to 400 of these inference engines are included on the Versal AI Core devices.In between the blocks is a software programmable multi-terabit network-on-chip which enables data transfer between the engines, the memory, and the interfaces.“This has been one of the choking points for FPGAs in the past, routing between different logics could degrade the performance, and having an actual ASIC — ahardened network-on-chip topology — we can move the data at a very fast speed, like gigahertz kinds of speed,” said Ni.—  Software Programmability  While FPGAs traditionally had their own specialist programming languages, today’s AI accelerators will need to be accessible to both software and hardware developers and data scientists. Software Programmability “As a company, we have invested significantly in bringing up the ease of use of software tools and frameworks for AI and software,” said Ni, referring to Xilinx’s purchase of Chinese AI startup DeePhi last year.“If we want to go after AI and software developers, we have to support C, C++, OpenCV, OpenCL — those kinds of languages. Then for AI developers, new languages like Python, Caffe, TensorFlow. … We are transforming the way we are hiring more software [people] and investing into tools and frameworks.”Available solutions include Xilinx’s DNNDK and ML Suite platforms, which can compile deep learning frameworks for FPGA boards without writing any C code or RTL code, Ni said.“We will continue to enhance this in the future and import it to Versal,” he said.Versal currently comes with its own development environment with a software stack that includes drivers, middleware, libraries, and software framework support. The Versal AI Core and Versal Prime series will be available in the second half of 2019."
IoT Nets in Two-Horse LPWAN RaceRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334892,https://m.eet.com/media/1312576/LPWANIHScover-min.png,Internet of Things Designline,07.05.19,"SAN JOSE, Calif. — LoRa and cellular’s Narrowband-IoT (NB-IoT) are far ahead of a pack of low-power wide-area networks (LPWANs) staking out early design wins in the internet of things. The LTE-M version of 4G cellular is a distant third, and Sigfox trails, according to a new report from IHS Markit.The report suggests that a once wide-open field is beginning to narrow significantly. However, it’s still early days. IHS estimated that just 150 million LPWAN links were deployed in 2018, a figure that it expects to expand at a 63% compound annual growth rate to hit 1.7 billion links by 2023.It’s also worth noting that some alternatives are just emerging from the lab. For example, multiple vendors are shipping their first chips this year for a 900-MHz version of Wi-Fi called HaLow that’s expected to hold significant promise for long-range connections. And last year, research institute CEA-Leti announced early work on a new option based on a patented Turbo-FSK waveform.That said, IHS forecasts that NB-IoT and LoRa could claim 86% of all LPWAN links in 2023. “We think it is a two-horse race by 2023, with LoRaWAN more in private and NB-IoT mainly in public networks,” said Christian Kim, one of the authors of the report.Interestingly, Huawei’s HiSilicon division is the leading supplier of today’s NB-IoT chips, 90% of which are deployed in China. Taiwan’s Mediatek is second, and China’s RDA Unisoc is third in NB-IoT silicon. NB-IoT had its origins in technology from U.K. startup Neul, acquired by Huawei in 2014.The relatively sluggish market for LTE-M chips is led by Qualcomm, followed by Sequans and Altair. Semtech is by far the dominant supplier of LoRa chips, the current market leader among LPWANs.Overall, 54% of last year’s LPWAN deployments were in China versus about 23% each in the Americas and the EMEA region. Government-backed smart-city projects in China are driving NB-IoT today with deployments in smart meters, parking meters, and streetlights.“Most projects are using government money,” said Kim. “A lot of enterprises have not warmed to NB-IoT, even in China.”To some extent, the LPWANs are solutions seeking problems. IHS currently tracks 20 use cases proposed for the links.The China government is promoting use of NB-IoT in smart homes for electronic locks, smoke detectors, and other uses, and Huawei has talked about its use in agriculture for more efficient dairy farms. However, it has not yet taken hold in such applications to date, Kim said.In the U.S., Wi-Fi and Bluetooth by far dominate in the smart home. And in developed countries, dairy farms are already operating at high efficiency levels, he added.Next page: Sequans, IHS have different views of LTE-MNext page: Sequans, IHS have different views of LTE-M"
NYU Abu Dhabi Chip Processes Encrypted DataNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334893,https://m.eet.com/media/1312580/CoPHEEexperimentalsetup_image1.jpg,MCU Designline,07.05.19,"Researchers at NYU Abu Dhabi (NYUAD) have designed a co-processor that relies on partially homomorphic encrypted (PHE) execution, enabling it to perform computations directly on encrypted data.Processors in PCs and smartphones currently compute on ordinary, unencrypted data only. The new processor, CoPHEE, mitigates data leakage and limits threats and vulnerabilities from hackers, by computing directly using encrypted data without decryption. The project is led by NYUAD assistant professor of electrical and computer engineering Michail Maniatakos, with contributors including research engineers at NYUAD’s center for cyber security (NYUAD CCS) Mohammed Nabeel and Mohammed Ashraf, NYUAD CCS post-doctoral associate Eduardo Chielle, and NYU alumni and assistant professor of electrical and computer engineering at the University of Delaware, Nektarios Tsoutsos. The project is funded by GlobalFoundries, which is owned by Mubadala, an investment firm based in Abu Dhabi.In a paper presented earlier this year at the IEEE International Symposium on Hardware Oriented Security and Trust (HOST), the NYUAD researchers said ASIC designs for encrypted execution impose unique challenges. They include the the need for non-traditional arithmetic units (modular inverse, greatest common divisor), very wide datapaths (2048 bits), and the requirement for secure multiplexer units enabling general-purpose execution on encrypted values. Even solutions like Intel SGX require the data to be processed as plaintext, which renders the entire microprocessor core and cache memories vulnerable to hardware Trojans and side channel attacks.To address this, the CoPHEE processor enables PHE encryption execution. It is a fully functional co-processor chip, and communicates to a main processor via UART. It was fabricated at GlobalFoundries in a 65nm CMOS process. Specifically, the designers used the multi-project wafer (MPW) fabrication service from MOSIS. The IC has a die area of 9mm2 and a target frequency of 100 Mhz (constrained by the maximum speed of the provided I/O pads).The researchers said that if a system-on-chip approach is taken where CoPHEE is also located on the same bus, the communication with the main CPU would clearly be much faster than this experimental off-chip set-up. Assuming a 32-bit ARM architecture, on-chip communication on AHB-Lite would accelerate communication to around 9.65E-08 seconds per operation.The processor is instantiated using 2048-bit encrypted operands and can be readily used to accelerate a broad range of secure applications, such as voting protocols, threshold cryptosystems, watermarking and secret sharing schemes, as well as server-aided polynomial evaluation protocols. For this it incorporates special arithmetic units for modular multiplication (ModMul), exponentiation (ModExp), inversion (ModInv) and greatest common divisor (GCD). In addition, to extend support for ciphertext-based control flow decisions in PHE-protected algorithms, it adopts the Cryptoleq blueprint and instantiates a secure multiplexer in trusted hardware, effectively minimizing the required trust surface to a single operation.The arithmetic units for modular multiplication, exponentiation, inversion, and GCD accelerate the computation of very wide datapaths, while its secure multiplexer and true random number generator enables universal computation in the encrypted domain. In their paper, the team conclude, “To the best of our knowledge, CoPHEE is the first academic effort towards constructing a fast and reliable processor capable of processing encrypted data. This paper presents all required steps for a fully functional silicon, from the RTL design to fabrication and validation. Given the silicon, future work will explore side-channel analysis and information extraction through power, timing, and electromagnetic emissions.”Maniatakos adds, “Existing data protection solutions protect data at rest in our hard disks and data in transit over the internet, similar to Whatsapp’s end-to-end encryption. These solutions are not suitable to manipulate encrypted data i.e. perform operations directly on the encrypted domain. With this new processor, non-trivial encrypted data manipulation is a reality and anyone stealing our data from our computers can do nothing with it since everything is encrypted. We are confident that any smart technology using data can benefit from the new processor including PCs, personal tablets, and smartphones.”Next page: Homomorphic EncryptionNext page: Homomorphic Encryption"
Rockley Raises $52M for Silicon Photonics TechSally Ward-Foxton,https://www.eetimes.com/document.asp?doc_id=1334890,https://m.eet.com/media/1312568/CARORockleyPhotonicsTROSAOFC191.jpg,Internet of Things Designline,07.04.19," Transmit-receive optical subassembly (TROSA) for data communications using Rockley Photonics’ silicon photonics platform (Source: Rockley Photonics)  Transmit-receive optical subassembly (TROSA) for data communications using Rockley Photonics’ silicon photonics platform (Source: Rockley Photonics) LONDON — Rockley Photonics has raised $52 million in a first closure of its Series E funding round, from both current investors and new investors, including Morningside Technology Ventures of Hong Kong. Rockley’s silicon photonics technology builds optical transceiver components in silicon and co-packages them with digital electronics, such as a switching ASIC. This latest funding brings the total raised by Rockley to $165 million.Silicon photonics is particularly attractive for the data center market, where intra-data center links require increasing bandwidth and speed. Rockley is also targeting consumer sensors and LiDAR applications.Of the $52 million total, $30 million came from Rockley’s strategic partner, Hengtong Opto-Electric Co, a Chinese manufacturer of fiber optic cables. The companies have a joint venture, in Suzhou, China, where they manufacture and sell 100G and 400G DR4 transceivers for data centers using Rockley’s LightDriver Engine.“[Hengtong’s investment] is a direct investment in Rockley Photonics to support the entire business plan,” said Rockley Photonics’ CEO, Dr. Andrew Rickman. “There will likely be a portion of this amount going to the joint venture to support our activity there. The market outlook for 100G, and higher, in the Chinese market remains strong and will be well served by our joint venture.”Rickman also said that the money raised so far was “halfway to our $100 million goal for this round,” adding that the company is growing across the board, including its UK presence in Oxford.In an earlier interview with EETimes, Rickman described the company’s competitive advantage as a combination of several factors.The company has developed a unified silicon photonics platform that can be used across multiple market segments, including consumer sensors, LiDAR, and data center connectivity, he said.“Our value proposition is multifaceted. One of these facets is the versatility of the platform and the optimization vectors we can manipulate in both the electrical and optical domains, which makes it suitable for all of these market segments thereby helping us drive economies of scale,” he said.Rockley’s portfolio has been developed around optimal solutions for both optical and electrical domains, as well as at the interface of the two.“This drive, coupled with our intense system-level thinking regarding in-package optics, has positioned Rockley’s LightDriver approach as the most practical solution from a system perspective,” he said.Rickman added that Rockley’s in-package optics solution is switch agnostic: an advantage of the company’s systemic approach to in-package optics and silicon photonics in general, compared to other offerings.“This makes us an attractive partner for multiple customers on the switch and ODM side,” he said.According to Rickman, 400G DR4 transceivers using Rockley’s LightDriver technology will start shipping at the end of the year. "
Startup Automates Type 1 Diabetes ManagementNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334891,https://m.eet.com/media/1312575/Diabeloopdiabetesmanagementsystem.png,Medical Designline,07.04.19,"Dealing with chronic disease can be a big chore both for affected patients. As Erik Huneker, CEO of French medical device startup company Diabeloop said last week at the Leti Innovation Days in Grenoble, “A person with type 1 diabetes needs to think and make a decision about 40-50 times a day about how much to eat and how much insulin to take.”This makes management of diabetes a perfect candidate for the utilization of artificial intelligence. That’s why Huneker’s company has developed what he claims is the world’s first autonomous medical device, which uses machine learning in real time to determine the optimum insulin to administer automatically to a patient without them having to even think about it.Type 1 diabetes affects about 2 million people in Europe and what Diabeloop aims to do with its closed loop medical system is to help people with the condition try and live normal lives by improving their treatment dramatically.Huneker emphasized they’re not simply developing an app. Its system, the DBLG1, comprises a continuous glucose sensor, a patch insulin pump and a dedicated handset that houses the Diabeloop algorithm (loop mode) and serves as a user interface with the system. The handset has its own custom operating system and connects with the sensor and the insulin pump using  secure Bluetooth.The system senses and monitors the patient’s blood glucose every five minutes, analyzes the data in real time, and can inject the correct does of insulin before the patient even knows he or she needs insulin, since it monitors daily patterns and over time will have learned the optimal parameters for the patient. The continual assessment takes into account current glucose levels, glucose trend, the hypoglycemic risk (short and long term) and special events such as meals, physical activities, and calibration, to provide an optimal recommendation adapted to the needs of the patient.Huneker said, “For each person the system runs 150 million equations per day for the machine learning.”The company’s DBLG1 obtained CE marking certification in November 2018, and earlier this year entered into a mutual distribution agreement with Netherlands-based ViCentra, which provides the insulin delivery system used as part of Diabeloop’s product.The results of a trial with 68 patients was published in the Lancet Digital Health journal. It found that the DBLG1 system improves glucose control compared with sensor-assisted insulin pumps; it concluded that its finding supports the use of closed-loop technology combined with appropriate health care organization in adults with type 1 diabetes.One of the outcomes of the study was that patients spent 68.5% of their time in the target glucose range, meaning a 10 point increase in the time spent in the target glucose range. There was also a decrease by 50% of time spent in hypoglycaemia (<0.7 g / l), in other words, a gain of more than 30 minutes less per day in hypoglycaemia. A third conclusion was that a reliable closed-loop system that was functional for more than 84% of the time in a home setting for 12 weeks.While this is focused on adults and the system is already being used by around 30 patients, the company is now also looking to adapt its ‘artificial pancreas project’ to juvenile diabetes and thus improve the quality of life for each child in the short term and the long-term. A study will be conducted in two centers in France and one in Belgium. The goal is to include around 20 children who will be followed for 6 weeks.We asked Huneker why his company was using a standard handset rather than developing its own smaller integrated device and possibly its own system on chip. He stressed that to do that could have needed years of development and the company really needed to get something to market that could be trialed on patients. This is why they are using standard off-the-shelf components – like the Dexcom G6 continuous glucose monitoring sensor system and the Kaleido insulin pump.Huneker said that the big challenge right now is to be able to do more on the device. “We need more computing capability.” He suggested they are planning to integrate more sensors and more computing power, and maybe three years from now could even think about developing a more integrated system that could be incorporated in a wearable device.Diabeloop emerged from a medical research project initiated by diabetologist Dr Guillaume Charpentier in 2011; the company was established in 2015 with support from CEA-Leti in Grenoble to help develop the product. The company now has around 50 people and has raised €13.5 million to date."
"HP, Dell Reportedly Shifting Manufacturing From ChinaDylan McGrath",https://www.eetimes.com/document.asp?doc_id=1334888,https://m.eet.com/media/1312561/HP_650_1-min.png,Industrial Control Designline,07.03.19,"SAN FRANCISCO — PC heavyweights HP and Dell are looking to move substantial production capacity out of China amid a bitter trade war between the U.S. and China, according to a report by the Nikkei news service, which cites unnamed sources.RecommendedChipmakers Get Glimmer of Hope on Trade WarRecommendedChipmakers Get Glimmer of Hope on Trade WarFellow tech giants Microsoft, Amazon, Google, Nintendo, and Sony are also looking to shift production of game consoles and smart speakers out of China, according to the report. Other PC OEMs, including Lenovo, Acer, and Asustek Computer, are also studying plans to shift manufacturing.The implication is that high-tech companies are moving production of goods headed for the U.S. market out of China to avoid the tariffs imposed on Chinese imports by the Trump administration. The Nikkei report quotes an unnamed supply-chain executive who said that the industry consensus is to move about 30% of production out of China, depending on how important the U.S. market is to the particular company.HP and Dell are the world’s No. 1 and No. 3 PC vendors, respectively. About 47% of Dell’s laptop PCs and about 40% of HP’s are exported to the North American market, according to the report, which cites market research firm Trendforce.According to a separate report by Trendforce, server ODMs are shifting some production from China to Taiwan in an effort to avoid the 25% tariffs imposed by the Trump administration on servers from China. Quanta, Wiwynn, Inventec, Foxconn, and others are adding production lines in Taiwan. Plans for products shipped to non-U.S. regions by these companies will remain as is.According to Mark Liu, a senior analyst at Trendforce, Quanta is the most aggressive of the server ODMs in adding manufacturing in Taiwan. Quanta’s goal is to add three new L6 server production lines to meet the future demands of 5G-related applications and data center growth, he said.Inventec has already added two new production lines in the Guishan district in the northern Taiwan city of Taoyuan, and Wiwynn already introduced a new production line for its plant at Southern Taiwan Science Park last year, according to Liu.In the long term, though, Trendforce said that it is still more economically beneficial for server ODMs to manufacture in China, even after adding tariff costs.  "
Are Emerging Memories Finally Emerging?Gary Hilson,https://www.eetimes.com/document.asp?doc_id=1334874,https://m.eet.com/media/1312445/cover-carousel-800x4751.jpg,Memory Designline,07.02.19,"TORONTO – Emerging memory technologies have been emerging for decades, but this year’s report by Objective Analysis and Coughlin Associates, Emerging Memories Ramp Up, suggests they have reached a critical point where they make sense in more applications than ever before.The report found that the emerging memory market is poised to reach $20 billion of combined revenues by 2029, largely by displacing today’s less efficient memory technologies such as NOR flash and SRAM—even displacing a share of DRAM sales. They will also be more competitive with incumbent memory technologies, as future process shrinks and improved economies of scale will drive down prices, enabling adoption both as standalone chips and embedded within ASICs, microcontrollers, and even compute processors. The three key emerging memories to keep an eye on are PCRAM, MRAM, and ReRAM; NOR flash and SRAM are prime candidates to be replaced. (Source: Coughlin Associates)  The three key emerging memories to keep an eye on are PCRAM, MRAM, and ReRAM; NOR flash and SRAM are prime candidates to be replaced. (Source: Coughlin Associates) But although the $20 billion figure sounds impressive, it’s important to remember that it’s the market for all emerging memories, with MRAM and PCRAM primarily in the form of Intel Optane being the dominant types, according to Jim Handy, principal with Objective Analysis. “DRAM reached the highest level that it may ever reach in its lifetime last year at just shy of $100 billion,” he said in a joint telephone briefing with co-author Thomas Coughlin, founder of Coughlin Associates.The report pegs the revenue growth of 3D XPoint — again, PCRAM in the form of Intel Optane — at $16 billion by 2029, thanks to its sub-DRAM prices. Stand-alone MRAM and STT-RAM revenues, meanwhile, will approach $4 billion, or more than 170 times MRAM’s revenues in 2018.  Along with ReRAM, MRAM is expected to compete to replace the bulk of embedded NOR and SRAM in SoCs, fueling even greater revenue growth.Emerging memories cover a breadth of technologies, but the key ones to watch out for are MRAM, PCRAM ,and ReRAM, said Coughlin. There’s been discrete MRAM devices out for some time, but there’s been a lot of talk about foundries building ASICs with specialized chips and replaced volatile memories with non-volatile options, he added. “That's going to be one of the big biggest drivers.”Handy said NOR flash’s inability to scale past 28 nanometers is also shifting attention to alternatives. “In the past, the only reason to have an emerging memory in an embedded application like on an MCU or on an ASIC is because you needed some technical attributes that it had, but it always added cost.” The prospect of being able to replace NOR flash using a smaller process node is spurring interest, he said.Overall, the economics of the emerging memories are also improving, said Coughlin, as foundries don’t necessarily have to add another back-end process but make it a part of the existing CMOS processing, which lowers the manufacturing costs as volumes go up and yield improves. He said it’s a “golden age for MRAM” because it’s getting an opportunity to prove itself in volume as foundries are looking to build it into embedded chips, while PCRAM has some support behind it, thanks to Intel Optane. Meanwhile, ReRAM is getting a lot of attention for artificial intelligence and machine learning applications. Even FRAM remains in the mix, as a dark horse contender for some applications.However, Handy believes it’s too early to name a winner,  and despite the healthy forecast ahead for emerging memory technologies, it’s still hard for them to get a leg up on entrenched technologies. “They're really hard to knock out of their leadership position even if the economics are improving. If you're not the cost leader then all these wonderful technical benefits you have over the entrenched technologies don't really mean an awful lot.”"
Chipmakers Get Glimmer of Hope on Trade WarDylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334885,https://m.eet.com/media/1312516/190702_Trump_xi_G20_800_2-min.png,Automotive Designline,07.02.19,"SAN FRANCISCO — Chip stocks surged Monday, the first day of trading after President Trump and China President Xi Jinping met at the G20 Summit in Osaka, Japan, and agreed to new trade negotiations and a hold on imposing new tariffs. Intel and Qualcomm rose, but the companies that are noted suppliers to Huawei, such as Broadcom, Skyworks Solutions, Qorvo, and Micron Technology, especially needed the rebound. Overall, the Philadelphia SOX semiconductor index rose nearly 2.7% Monday.“The progress made today by President Trump and President Xi in Osaka is good news for the semiconductor industry, the overall tech sector, and the world’s two largest economies,” said Jon Neuffer, president and CEO of the Semiconductor Industry Association (SIA), in a statement released on Saturday. “We are encouraged [that] the talks are restarting and additional tariffs are on hold, and we look forward to getting more detail on the president’s remarks on Huawei.”RecommendedBroadcom Raises a $2B Trade War WarningRecommendedBroadcom Raises a $2B Trade War WarningDays later, detail remains lacking. The semiconductor industry is relieved now that talks between the U.S. and China are resuming, but of course, the outcome of new trade negotiations is far from assured.On Saturday, Trump referenced lobbying efforts on behalf of U.S. component suppliers regarding the Huawei ban and said — in an apparent overture to Xi — that U.S. companies could sell their “high-tech” products to Huawei as long as it didn’t create a “great, national emergency problem.”It remains unclear what products U.S. companies will be able to resume supplying to Huawei, which was placed on an export blacklist last month over U.S. national security concerns.Analysts and semiconductor industry insiders were scrambling Monday to try to interpret Trump’s comments on Huawei. Trump administration officials said Monday that the most likely scenario would be the issuance of more licenses to enable U.S. firms to sell some products to Huawei.The renewed trade talks and Trump’s apparent about-face on Huawei come two weeks after the Office of the U.S. Trade Representative wrapped up a series of hearings on the administration’s proposal to impose tariffs on about $300 billion more of Chinese products. Not surprisingly, U.S. companies from Apple to Walmart have come out in opposition to tariffs, with more than 500 companies and 140 industry groups signing an open letter to Trump earlier this month asking him to reconsider his tariffs.On June 19, four traditional U.S. PC industry heavyweights — HP, Dell, Microsoft, and Intel — banded together to issue a joint statement, saying that though they recognize and appreciate Trump’s efforts to address unfair practices, the proposed tariffs on laptops and tablets “threaten to disproportionately harm multiple U.S. interests, including small and medium-sized businesses, a wide range of consumers, and device manufacturers.” The group, citing a study released earlier this month by the Consumer Technology Association, said that the proposed tariffs would increase U.S. prices for laptops and tablets by at least 19%, or about $120.The SIA, like virtually every U.S. trade group, supports Trump’s aims of cracking down on China’s trade practices — including forced technology transfers and lax IP protection — but continues to argue that the tariffs imposed on semiconductors and the semiconductor supply chain hurt U.S. chipmakers and do nothing to curb China’s trade practices at issue.Devi Keller, director of global policy at SIA, testified at one of the U.S. Trade Representative hearings that the tariffs were ill-equipped to address the issues at hand. Keller testified that the semiconductor test, packaging, and assembly performed in Asia — including China — represents only about 10% of the value of a semiconductor. Additional tariffs, she argued, could have crippling effects on the U.S. semiconductor industry and U.S. IT as a whole.“What we’re looking at now, with all four [tariff] lists, is you’re seeing the entire semiconductor ecosystem covered by tariffs both downstream and upstream, and our view is that’s going to have a very crippling impact,” Keller said in a subsequent interview with EE Times."
EU AI Will Rely on MemoriesNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334886,https://m.eet.com/media/1312554/StanfordSubhasishMitraN3XT.jpg,Internet of Things Designline,07.02.19,"Imec, the Belgium-based nanoelectronics and digital technologies research center, is leading a European Union program to develop low-power edge artificial intelligence (AI) chips based on several emerging memory technologies.The three-year program, called Tempo (Technology & hardware for nEuromorphic coMPuting), is a cross-border collaboration between 19 research and industrial partners, including CEA-Leti of France and the Fraunhofer Group of Germany. The joint effort aims to develop process technology and hardware platforms leveraging emerging memory technologies for neuromorphic computing. The goal is to develop a new way to support applications in mobile devices that need complex machine-learning algorithms.Today, applications of this sort typically rely on shipping data to cloud-based server racks and then back. Complying with European data privacy regulations is difficult with a cloud-based approach however. Given those restrictions, the alternative is to perform AI on the edge — within battery-powered mobile devices such as cars and smartphones. The technology to do so doesn't exist, so Europe must create it.The topic is relevant especially in Europe, but it's a concern across the industry. Edge AI and machine-learning algorithms are becoming increasingly necessary in day-to-day products and applications such as smart home assistants with natural-language processing, security systems that employ facial recognition, or autonomous vehicles. The demand for complex computational algorithms will only grow further.There are plenty of other reasons to avoid using the cloud beyond complying with data privacy rules. Sending data to the cloud costs energy and latency. The ultimate for edge AI applications is to enable intelligent energy-efficient local processing.Tempo aims to evaluate current solutions at device, architecture and application level, and build and expand the technology roadmap for European AI hardware platforms. The project will leverage MRAM (imec), FeRAM (Fraunhofer) and RRAM (CEA-Leti) memory to implement both spiking neural network (SNN) and deep neural network (DNN) accelerators for eight different use cases, ranging from consumer to automotive and medical applications.The director of the Fraunhofer Institute for Photonic Microsystems (IPMS) and chairman of the board of directors of the Fraunhofer Group Microelectronics, Prof. Hubert Lakner, said a key enabler for machine learning and pattern recognition is the capability of the algorithms to browse through large datasets. In terms of hardware this means having rapid access to large memory blocks. Therefore, one of the key focal areas of Tempo are energy efficient nonvolatile emerging memory technologies and novel ways to design and process memory and processing blocks on chip.Emmanuel Sabonnadiere, CEO at CEA-Leti said they aimed to sweep technology options covering emerging memories and attempt to pair them with contemporary (DNN) and exploratory (SNN) neuromorphic computing paradigms. The process and design compatibility of each technology option will be assessed with respect to established integration practices and industrial partner roadmaps and needs to prepare the future market of edge AI “where Europe is well positioned with multiple disruptive technologies.”Luc Van den hove, CEO at Imec, commented, “We are delighted to enter in such broad European collaboration effort on edge artificial intelligence, gathering the relevant stakeholders in Europe, including CEA-Leti and Fraunhofer, two of our most renowned colleague research centers in Europe. Thanks to our combined expertise, we can scan more potential routes forward than what would be possible by each of us individually, and as such, position Europe in the driver seat for R&D on AI.” He added that behind the scenes, they are already defining more public and bilateral agreements with several of the partners involved.Tempo is a European innovation project funded by the ECSEL Joint Undertaking (JU), a public-private partnership for electronic components and systems which funds research, development and innovation projects in key enabling technologies. The JU receives support from the European Union’s Horizon 2020 research and innovation program, and from Belgium, France, Germany, the Netherlands, and Switzerland.Tempo, which was officially kicked off in April 2019, will run for three years. The consortium consists of nineteen members, with Imec taking the lead as the sole Belgian consortium partner. The other consortium members are, for France: CEA-LETI, ST-Microelectronics Crolles, ST-Microelectronics Grenoble, Thales Alenia Space and Valeo. For Germany: Bosch, Fraunhofer EMFT, Fraunhofer IIS, Fraunhofer IPMS, Infineon, Innosent, TU Dresden and Videantis. For the Netherlands: imec the Netherlands, Philips Electronics and Philips Medical Systems. For Switzerland: aiCTX and the University of Zürich."
China’s Tsinghua Launches DRAM UnitDylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334887,https://m.eet.com/media/1312558/Tsinghua_unigroup_800-min.jpg,Memory Designline,07.02.19,"SAN FRANCISCO — Chinese government-backed Tsinghua Unigroup has established a new DRAM unit in a renewed push to achieve semiconductor independence amid ongoing friction with the U.S.The new memory unit is led by CEO Charles Kao, chairman of Inotera Memories and former president of Nanya Technology, both of which make DRAM. Kao is also chairman of Yangtze Memory Technology (YMTC), another Tsinghua Unigroup venture attempting to produce NAND flash memory.RecommendedTsinghua Lures SMIC Co-CEO Zhao to Bolster DRAMRecommendedTsinghua Lures SMIC Co-CEO Zhao to Bolster DRAMChina’s stated goal is to domestically produce 70% of the semiconductors needed to supply its massive internal electronics markets by 2025. The country's push to bolster its domestic semiconductor production faces renewed urgency amid a trade war between the U.S. and China. Last month, the Trump Administration placed Chinese telecom giant Huawei Technologies, one of the largest companies in China, on an export blacklist preventing U.S. companies from supplying it with chips and other components. For the Chinese, the blacklisting of Huawei emphasized the importance of developing domestic supply sooner rather than later. Another Chinese DRAM hopeful, Fujian Jinhua Integrated Circuit Company (JHICC), ceased DRAM development earlier this year. JHICC was placed on the U.S. export blacklist last year, preventing U.S. chip equipment firms such as Applied Materials and KLA-Tencor from supplying it with equipment, after it was indicted by the U.S. for alleged industrial espionage and intellectual property theft.In March, EE Times reported that Tsinghua was looking to hire Haijun Zhao, the co-CEO of Chinese foundry Semiconductor Manufacturing International Co. (SMIC), to head up a new DRAM company that would combine China’s fledgling memory makers. However, Zhao has remained in his post at SMIC and does not appear to be associated with the new Tsinghua DRAM unit.According to TrendForce, a Taiwan-based market research firm, the new DRAM unit will benefit from YMTC’s fab-building experience and Tsinghua’s relationship with subsidiary Unigroup Guoxin Microelectronics Co., which has also pursued DRAM development and may be assimilated into the new unit. Tsinghua is in the process of constructing a fab in Nanjing in Eastern China and is also working closely with Chongqing, another possible fab location, TrendForce said.The firm said that the new group will likely rely on Kao’s experience and industry connections to develop DRAM process technology. "
"As Trade Talks Resume, China Advances 5G Junko Yoshida",https://www.eetimes.com/document.asp?doc_id=1334879,https://m.eet.com/media/1312474/US-Chinatradetalktable-min.jpg,Wireless and Networking Designline,07.01.19,"For the global semiconductor industry, the just-revealed truce in the “trade war” between U.S. President Donald Trump and China’s president Xi Jinping is welcome news but hardly the end of a poorly scripted economic melodrama that continues to pose dire consequences for the American semiconductor market.In a quiet victory for China and its 5G ambitions, Trump and Xi agreed Saturday to resume trade talks that had collapsed in May. Concessions offered by President Trump include no new tariffs and an easing of restrictions on trade with Huawei, in exchange for China making unspecified new purchases of U.S. farm products.The outcome of the negotiations between Trump and Xi in Osaka is not exactly a big breakthrough. It is said to be broadly in line with market expectations ahead of the summit. Removal of the immediate risk of much higher tariffs, however, is likely to generate a relief rally in the financial markets. [ As of mid-day Monday, most U.S. exchanges and indices were up no more than 1 percent — ed. ]As of mid-day Monday, most U.S. exchanges and indices were up no more than 1 percent — ed.The U.S. Commerce Department, according to Trump, will “study in the next few days whether to take Huawei off the ‘entity list.’” The entity list is a de facto blackballing of the Chinese telecom equipment giant; U.S. companies have been banned from selling parts and components to Huawei without U.S. government approval. Some key international vendors had also agreed to cut off sales to Huawei.de factoTwo weeks ago, Broadcom surprised the industry by revealing that the U.S.-China trade tensions and the ban on doing business with Huawei would knock $2 billion off the company’s sales this year.The global chip market to shrink by $100 billion in 2019 Last week at an event held by CEA-Leti in Grenoble, France, Handel Jones, the founder and CEO of International Business Strategies, Inc., warned the audience, “If the U.S. and China do not find a way to ease the current trade tension, we are expecting the global semiconductor market in 2019 to shrink by 18 percent — roughly $100 billion — compared to 2018.”The global chip market to shrink by $100 billion in 2019——After the so-called ""truce"" was announced by President Trump, Jones told EE Times, ""The data (above) I shared in Grenoble is based on tariffs for $300 billion, which is [now] postponed."" Noting that IBS is currently redoing the chart, Jones said he estimates a decline [of the global chip market] to be closer to $80 billion, instead of $100 billion. ""It is still a large decline,"" he added. ""Most of the decline comes from dips in DRAM and NAND."" EE TimesThe latest cease-fire in the trade war between the world’s largest nations might narrowly avoid immediate disaster. But is returning to the negotiation table — a low bar — enough to remove the business community's concerns over the uncertainty between the two countries? Executives in the chip industry often complained that it is ""this uncertainty — not knowing what Trump does next"" that's killing their business.———Further, it remains unclear how this will eventually lead to any resolution on the fundamental conflict between the two nations. The United States’ concerns on China range from issues of intellectual property thefts and China’s industrial policy to Huawei’s close relationship with Beijing and potential national security risks posed by Huawei’s next-generation 5G telecommunication infrastructure equipment. China, meanwhile, is asking for the U.S. to treat Chinese companies fairly and show mutual respect.It appears that the U.S. President, eager to use Huawei as a bargaining chip for short-term leverage, is trading away his administration’s original concerns over national security and IP thefts.Jones observed that Senator Marco Rubio is trying to tighten the restrictions on shipping products to Huawei and he has support in the Senate as well as in Congress. The reality is, said Jones, ""Nothing is firm and everything is up for negotiations.""""IBS' position is that China and Huawei will be hurt in the next 2 to 3 years, but longer term, China will be stronger,"" noted Jones.Future of 5G One thing is clear. As IBS’ Jones noted, the race to 5G technology has been already won by Huawei in China (with Ericsson as runner-up and Nokia a distant third). The ongoing tension between the United States and China is not helping the U.S. either to kickstart 5G domestically or take the lead in the global 5G race.Future of 5GAsked for proof that China has won the still emerging 5G market, Jones told us that Huawei has already made more than 50,000 5G base station equipment sales to all the leading telecom operators in China and broadcasters. Early installations of 5G telecom equipment in the field are helping Huawei gain in-depth insights and learn lessons fast, he told EE Times.Most likely, global 5G market will be split in two — with China honing, advancing and creating its own 5G standards, Jones predicted. And the United States? “We might have to license IPs on 5G from China.”—— Junko Yoshida, Global Co-Editor-In-Chief, AspenCore Media, Chief International Correspondent, EE Times EE Times"
Applied Bids $2.2B for KokusaiRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334882,https://m.eet.com/media/1312480/AMATKElogosinvert-min.png,SoC Designline,07.01.19,"SAN JOSE, Calif. — Applied Materials, Inc., bid $2.2 billion to acquire Kokusai Electric Corp. in a deal in line with the current conservative tone of acquisitions in the capital equipment sector. Kokusai’s vertical diffusion furnaces, which are mainly used by DRAM and NAND vendors, would add about 10% to Applied’s revenues.Kokusai grew about 42% in the memory boom of the past three years and logged an estimated $1.8 billion in 2018 revenues. But it is likely to take a significant hit from the current downturn among DRAM and NAND makers who are its major customers.Given its relatively modest size, the deal is likely to escape becoming another pawn in the China/U.S. trade war, analysts said. It requires regulatory approval in China, Ireland, Israel, Japan, and Korea but not in the U.S.“Applied has a long history and strong relationships in China — I visit there frequently — and we think the deal will be treated fairly there,” said Gary Dickerson, chief executive of Applied. He declined to break out Kokusai’s China revenues.Dickerson helped set off a series of bold but unsuccessful merger attempts in the semiconductor capex sector. Soon after he joined Applied in 2013, it bid to merge with Tokyo Electron in a deal that would have united the sector’s first- and third-largest players. In early 2015, the U.S. Department of Justice nixed the deal.Eighteen months later, regulators struck down a $10.6 billion merger between KLA-Tencor and Lam Research, putting a chill on capex deals despite a historic period of consolidation in the maturing semiconductor industry overall.In March 2018, KLA bid $3.4 billion for Israel-based Orbotech in a deal that closed in February with approval from China and others. Analysts said that the deal gave Applied confidence to bid on Kokusai.“We think it is very possible that the two companies will secure the required approvals within the expected 12-month time frame, though we think there is some amount of risk that there could be regulatory difficulties,” wrote analyst David Wong of Nomura Instanet in a research note.About 80% of Kokusai revenues come from tube chemical vapor deposition furnaces, where it holds an estimated 58.6% market share, Wong wrote. Tokyo Electron is the other big player in the area with a 38% share, while Applied currently has no products in the so-called batch-processing market.Next page: Deal could spark Lam bid for ASMINext page: Deal could spark Lam bid for ASMI"
Chip Sales Fall for Fifth Straight MonthDylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334884,https://m.eet.com/media/1312512/semiconductor_1200_2-min.png,Automotive Designline,07.01.19,"SAN FRANCISCO — Global semiconductor sales declined on an annual basis for the fifth straight month in May but increased modestly on a sequential basis as the chip industry downturn continued amid an ongoing trade war between the U.S. and China.Chip sales fell to $33.1 billion in May, down 14.6% from the May 2018 total of $38.7 billion, according to the World Semiconductor Trade Statistics (WSTS), an organization of 45 major semiconductor industry member companies who pool sales data. Sales did improve by 1.9% compared to April’s total of $32.5 billion.RecommendedQ1 Chip Sales Drop Among Largest on RecordRecommendedQ1 Chip Sales Drop Among Largest on Record“Global semiconductor sales fell well short of last year’s monthly total in May, marking the fifth straight month of negative sales growth on a year-to-year basis,” said John Neuffer, president and CEO of the Semiconductor Industry Association, in a press statement. “On a month-to-month basis, global sales increased modestly and sales into the Americas increased for the first time in seven months, although year-to-year sales into the Americas were down substantially.”Sales declined on a year-to-year basis across all regions of the world, according to WSTS. On a month-to-month basis, sales increased in China, the Americas, and Japan but fell in all other regions.The semiconductor industry is widely expected to contract in 2019 after three straight years of market expansion as the memory boom that drove much of the growth in recent years has turned to bust and the world’s two largest economies continue to impose tariffs on each other’s exports. The WSTS is currently forecasting that global semiconductor sales will decline 12.1% to about $412 billion in 2019. "
Data Privacy Splits Global AI RaceJunko Yoshida,https://www.eetimes.com/document.asp?doc_id=1334876,https://m.eet.com/media/1312453/EmmanuelSabonnadiere_1200_nu-min.jpg,Internet of Things Designline,06.29.19,"GRENOBLE — In the world of AI research, Europe has drawn a line in the sand, declaring that R&D must focus squarely on “Edge AI.”This proclamation draws a stark contrast to “Cloud-based AI,” the model aggressively pursued by China and the United States. During “Innovation Days” hosted here by French research institute CEA-Leti this past week, Emmanuel Sabonnadiere, CEA-Leti’s CEO, discussed the “two schools of AI research” that have split the world in two.  Both the U.S. and China have been collecting massive amounts of data which they use for training AIs, the basis for their claims they lead the world AI race. Strict data privacy regulations in Europe might be seen as impeding European companies' progress in AI, but that's not necessarily the case. Conforming to those regulations is instead  “shaping Europe to manage its AI research strategy very differently,” said Sabonnadiere. Once AI is trained in the cloud, Europe sees its role as applying further learning and personalization to “Edge AI.”Is Europe behind in AI? This prompts the question of whether Europe is really lagging in the race for AI. The answer: not necessarily. To begin with, from a worldwide perspective, AI research is barely out of the starting gate.Is Europe behind in AI?It's easy to dismiss the notion of ""Edge AI,"" given that it's something plenty of companies say they're already doing. That's true to the extent that everybody seems to be slapping AI accelerators into smartphones and calling it “AI at the edge.”  To be clear, when CEA-Leti talks about ""Edge AI,"" this means its researchers are thinking about technology used in inferences that go way beyond current edge AI practices.Sabonnadiere explains that Edge AI is “a huge challenge,” one that Europe “absolutely must solve,” given that data privacy rules aren't going away.By definition, solving big challenges requires innovation. CEA-Leti and its researchers have laid out a 10-year roadmap for Edge AI. The technologies range from 3D stacking to in-memory computing and on-die integration of resistive non-volatile memories.  Advancements in such areas will help reduce energy per operation.The key to frugality with power consumption at the edge, said CEA-Leti's Francois Perruchot, is: “Do not move data,” from an external memory block to an AI processor. Every time data moves, AI power consumption spikes at the edge by 100 to 1000 times,"" he noted. Perruchot is director of strategic marketing on sensors at Leti.In parallel, reducing the number of AI operations at the edge is crucial. CEA-Leti is exploring a neuromorphic approach to data processing and spike-coding for deep neural network processing of sensor inputs.10 TOPS per watt The goal for CEA-Leti’s Edge AI research is to develop — over five years — “an Edge AI processor running at 10 tera-operations per second (TOPS) per watt,” said Sabonnadiere. This requires a combination of new memory architecture, spiking algorithms and sensor arrays. Once achieved, he said, “this will be a game changer.” It will be a sharp contrast to a typical current GPU running at 1 TOPS per 200 watts, according to CEA-Leti.10 TOPS per wattIn Sabonnadiere’s view, data privacy has created an R&D opportunity unique to Europe. This poses a constraint for European researchers, but it also forces them to tackle the issue head on, an arduous process the rest of the world has barely pondered. It remains to be seen if Edge AI will indeed enable Europe to win the global AI race, but at least it is a goal Europe can use to differentiate its AI research from the rest of the world.AI without hardware? CEA-Leti’s CEO regards as a huge asset its 50-year history of R&D deeply involved in testing and manufacturing of microelectronics.AI without hardware?AI research in Silicon Valley has profited hugely from algorithms advanced by technology platform companies such as Facebook, Google, Amazon and Microsoft. Further, Sabonnadiere acknowledged that AI research success by those Internet giants — “without spending big capital expenditures” — has spun the narrative that AI is ruled by AI software algorithms. However, he stressed that “AI without hardware” will eventually hobble AI’s potential.AI researchers in Europe are also cognizant that the overwhelming hype around AI among investors, media and the public could kill AI prematurely. Noting that the long history of AI has gone through cycles of “AI winter,” Sabonnadiere said, “We could face yet another ‘deep freeze.’”AI, he said, must stand on two pillars of discipline: ""Edge AI"" and ""Trusted AI.""AI that tells us ‘I don’t know’ By Trusted AI, he means, AI that respects privacy, can explain itself, and is responsible and reliable.AI that tells us ‘I don’t know’Patrick Gros, CEO of the Inria/Grenoble, French Institute for Research in Computer Science and Automation, explained AI bluntly. “AI isn’t intelligent. If there is any intelligence, it is in AI developers.” AI, in the simplest terms, can be explained as “brute force applied to data,” he noted. The probabilistic nature of AI could also make it problematic. The issue is not that AI makes the right decisions all the time. When the outputs of an AI decision miss predictions by a mile, “We need AI to tell us, ‘I don’t know,’” said Gros.AI’s confidence in its own decision-making greatly matters when AI is used in life-critical systems, be it an autonomous car, an airplane or a medical device.One good example is Diabeloop, a startup working in partnership with CEA-Leti. The company has developed a type-1 diabetes management system, already approved by regulators in Germany and France. It monitors a patient’s blood sugar and reproduces the pancreatic function. The startup’s CEO, Erik Huneker, told us that because diabetics have “immensely varied” lifestyles and needs for insulation injection levels, it’s important that the AI system “locally learn” the patient’s needs and “personalize” the system.Describing the system as “the first autonomous medical device that makes a decision,” Huneker said, “When AI’s decision deviates from predictions by 40 percent, the system shuts itself down. It would not send insulin automatically.” In other words, the device is run by two systems — an autonomous system to execute local learning and a deterministic system to prevent the AI-driven device from injecting the wrong insulin dosage.AI needs formal certification The factors that make AI system systems “less robust” and “unfair” are the incomplete data sets often used to train AI, Inria’s Gros added.AI needs formal certificationIt’s not enough for companies to say, “We did our best” to make AI reliable, stressed Gros. “We need to formally certify that [AI-driven] systems are fair.” Unlike smartphones that could miss an “event” or two in receiving sensory data, life-critical AI systems must be designed based on “ethical and legal AI frameworks,” said Gros. “And they must be “formally certified.”Next page: Global alliancesNext page: Global alliances"
5 Edge AI Projects Spotted at CEA-Leti EventNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334877,https://m.eet.com/media/1312462/spiritchip_1000-min.jpg,Internet of Things Designline,06.29.19,"GRENOBLE – CEA-Leti picked ""Deep tech for edge artificial intelligence (AI)"" as its theme for this year's Leti Innovation Days’ annual conference. The event last week, attended by 800 delegates from all over the world, took place during a blistering heatwave that saw outside temperatures of almost 40 degrees Celsius (104°F).Inside the Minatec campus, where lectures, workshops and demonstrations were held, attendees were reminded of significant challenges they would face by moving AI to the edge.Driving  such a move is none other than data privacy issues. Other factors prompting the industry to embrace edge AI, however, are their desire to reduce AI computation both in time and energy consumption. Data transfer and memory access would account for up to 90% of system energy usage, thus reducing data movement between memory and processor becomes critical.Recommended Data Privacy Splits Global AI Race Recommended Data Privacy Splits Global AI Race CEA-Leti, whose research focus includes health, environment and energy, strutted its stuff by showcasing a number of research projects and demonstrators during the event. Brought to the show floor were Leti's own research projects and others by startups working with or spun out of Leti.We will share five projects that caught our eyes in the following pages.Next page: Tracking Sleep Apnea in a WearableNext page: Tracking Sleep Apnea in a Wearable"
Private 5G Networks Expected to Boost Industrial IoTDylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334872,https://m.eet.com/media/1312438/Sensors_Expo_2019_800_2-min.png,Internet of Things Designline,06.28.19,"SAN JOSE, Calif. — The rollout of 5G technology will have sweeping implications for the industrial IoT but won’t impact all IIoT applications in the near term, according to Gerardo Giaretta, senior director of product management and the head of Industry 4.0 at Qualcomm.5G will offer enormous benefits in areas like smart factories but won’t immediately be a solution for applications that are relatively simple and require massive-scale deployment, such as smart meters and many farming and agricultural applications, he said in a keynote address at the Sensors Expo & Conference 2019 here Thursday.Recommended5G, AI are Weapons in a WWIIIRecommended5G, AI are Weapons in a WWIIIMany of these applications will continue to rely on low-power wide-area network (LPWAN) derivations of LTE technology such as LTE Cat M and NB-IoT, which are more tailored to IIoT applications than general-purpose LTE and more well-suited for those use cases that don’t take advantage of the increases in bandwidth and speed offered by 5G, Giaretta told an audience of several hundred engineers.“Many use cases require a cost structure that 5G may not be able to achieve,” at least not in the next couple of years.5G — the fifth generation of cellular technology — is just now at the earliest stages of deployment, with carriers just beginning to offer the service in some locations and the earliest 5G handsets hitting the market. As the rollout progresses over the next several months, it is expected to offer enormous advantages in latency and speed, bringing mobile broadband access to smartphones and other mobile devices. But the implications for IoT and particularly IIoT may be just as profound, if not more so.Giaretta and others believe that the biggest impact of 5G on the IIoT could be the enablement of 5G private networks operating on factory premises. Private 5G networks within factories could enable companies to set up secure, dedicated networks within limited geographic zones that could be tailored for specific industrial applications.Some factories already use private networks built on LTE technology. But the 5G system architecture was designed with the private network in mind, enabling the deployment of standalone private networks that don’t rely on external networks. And unlike LTE, 5G offers multiple options for allocating spectrum to private networks, including the acquisition of licensed spectrum in a specific geographic area from mobile operators, the use of unlicensed spectrum with both synchronous and asynchronous sharing, and, in some countries, spectrum that has been specifically allocated for IIoT use.“It’s really the first time where we will see industry players — or, for that matter, smart city players — that will be able to deploy their own network, and that will free up innovation,” Giaretta said.He emphasized that secure spectrum for private 5G networks will still carry costs but said that the new model of deploying 5G networks will differ substantially from the current model for private networks, which requires a long-term deal with a telecom operator.The model is going to be “very, very different” because industrial players deploying 5G private networks for IIoT will be able to plan for their 5G network 10 to 15 years into the future “because they know that they kind of own it.”According to Giaretta, “It’s really, really a big innovation. It will have the potential to change completely the dynamics of factory networks.” "
Sensors See Big-Data OpportunityRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334875,https://m.eet.com/media/1312446/SensormoduleST-min.jpg,Internet of Things Designline,06.28.19,"SAN JOSE, Calif. — A startup with a brainwave monitor powered by deep learning could help mobile and cloud-computing giants tap into your thoughts. The company was one of several device makers that responded at the Sensors Expo to a call from Google for smart home devices to support its Assistant service.Cloud services and smartphones probably won’t be monitoring thoughts anytime soon. However, the industry is clearly in a race up the food chain from core sensor technology to software and data, according to several vendors at the event.Petal uses off-the-shelf headsets that measure electroencephalogram signals that it processes with homegrown neural-network models written in Google’s TensorFlow framework. The two-year-old startup is currently demoing software that lets users play video games with their thoughts.The company said that Samsung responded positively to a demo that it conducted at Samsung’s Korean headquarters. The startup is seeking connections with Google’s home and embedded groups. The five-person company hopes to ship a developer’s kit within a month supporting Android, iOS, the Mac, and Windows.For its part, Google wants to expand its ecosystem of 35,000 devices certified to support the APIs for its Assistant service. Assistant is anchored in Google’s HomeGraph, a software construct that mines smart home data for insights.The service is “based on voice today, but the future is about contextual understanding and visuals, and that’s where more sensors come into play,” said Wendy Kam, a Google business development manager speaking at the event. “More things need to include sensors for the Assistant to offer a robust experience” that lets consumers personalize predictive services.So far, Google’s Assistant partners include Philips Hue in lighting and LG and iRobot in appliances, but “we are at the early stage of the Assistant ecosystem,” said Kam. “If you want your sensor to work with other devices, let us know the sensor and use cases so we can define traits and APIs.”Google competes with Apple, Amazon, and others to create a smart home experience. In China, Baidu was early to launch a similar effort. To date, no standards have emerged for devices to support a hybrid mix of services.Google aims to launch a home security service soon, roughly similar to Amazon’s Alexa Guard. The search giant is also driving a TensorFlow Lite initiative to put deep-learning models running on its own controller in just tens of kilobytes into low-latency embedded services that may not need to be linked to the cloud.Long term, the kinds of third-party add-on products that consumers deploy on doors and windows for security will become embedded in new homes and furnishings. “Companies come to us asking how to future-proof their businesses, and we recommend them to system integrators,” said Kam.It’s part of a broad trend of leveraging digital technology to create efficiencies and new revenue streams. “Everyone wants to move into the software-as-a-service model,” said one presenter at the event.Next page: STM, Omron move up to modules and softwareNext page: STM, Omron move up to modules and software"
AI Startup Preps Inference ChipRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334862,https://m.eet.com/media/1312401/israelAI-min.png,Wireless and Networking Designline,06.26.19,"SAN JOSE, Calif. — Add NeuroBlade to the dozens of startups working on AI silicon. The Israeli company just closed a $23 million Series A, led by the founder of Check Point Software and with participation from Intel Capital.NeuroBlade is close to taping out an inference processor that it aims to sample late this year or early in 2020. Following in the footsteps of Israeli AI startup Habana, it plans a training accelerator using a similar architecture for its next generation.The startup claims that its initial chip will run several neural networks and multiple algorithms in parallel. The chip supports a full range of neural-net models with fully connected layers, including CNNs, RNNs, LSTMs, and GANs.The chip will provide similar performance on inference jobs as Intel CPUs or Nvidia GPUs at a lower cost thanks to a much smaller die, according to the company. Alternatively, it will support significantly improved performance with a similar size and cost die.NeuroBlade declined to provide any public benchmark results until chips are shipping. However, it said that it is sharing performance metrics with select customers.Founders Elad Sity, CEO, and Eliad Hillel, CTO, met while working at Solar Edge, a startup that went public making inverters and other gear for photovoltaic arrays. Both are graduates of the technology unit of the Israel Intelligence Corps.NeuroBlade got its initial start with a $4.5 million seed round in 2017 from StageOne Ventures and Grove Ventures, headed by Dov Moran, the inventor of the USB flash drive and co-founder of M-Systems.Related posts:Related posts:"
"Micron Cuts Capex, Wafer StartsDylan McGrath",https://www.eetimes.com/document.asp?doc_id=1334863,https://m.eet.com/media/1312403/Micron_800-min.png,Memory Designline,06.26.19,"SAN FRANCISCO — U.S. memory chip supplier Micron Technology said that it would reduce its capital-spending plans for fiscal 2019 and 2020 and further trim wafer starts amid an ongoing demand slowdown that has thrown the memory market into a tailspin.Micron said that it would slash capital spending for its fiscal 2019, which closes in August, to about $9 billion, down about 10% from the company’s estimate of about $10.5 billion at the start of the fiscal year. Micron said that it expects its fiscal 2020 capital expenditures to be “meaningfully lower” than fiscal 2019.Micron President and CEO Sanjay Mehrotra announced that the company would also continue with a previously announced 5% cut in DRAM wafer starts, which company executives expect will bring the company’s DRAM bit supply growth for 2019 close to market demand growth.Micron said that it would further cut its NAND wafer starts from 5% to 10%, which will result in lower supply growth in the second half of the year. While the overall NAND market remains oversupplied from the accelerated supply growth driven by the industry transition to 3D NAND, Micron remains optimistic that the NAND market will start to stabilize in the second half of this year, Mehrotra said in a conference call with analysts.“With the higher levels of macro uncertainty and the relatively high levels of inventory on our balance sheet, we are taking decisive action to manage our DRAM and NAND bit production,” he said.Business with Hauwei resumes… a little Micron surprised analysts by saying that it has resumed in the past two weeks shipments of some products to China’s Huawei Technologies, which was placed on an exports blacklist by the Trump administration last month. Executives said that the company resumed shipments of some products after determining that these products are not subject to the restrictions imposed by the Trump administration.Business with Hauwei resumes… a little“However, there is considerable ongoing uncertainty surrounding the Huawei situation, and we are unable to predict the volumes or time periods over which we’ll be able to ship products to Huawei,” Mehrotra said.The announcements on capital expenditures, on wafer starts, and on Huawei came in the wake of Micron’s fiscal third-quarter financial results, which beat Wall Street’s expectations despite hefty declines in sales and profit.After robust growth amid rising average selling prices in both 2017 and 2018, the memory chip market has entered a downturn and is projected to decline by about 30% this year, according to market research firm IC Insights.“Although previously announced capex cuts will start to impact industry supply in the second half of the calendar year, our assessment is that further cuts in capex and bit supply will be required to return the industry to a healthy supply/demand balance,” Mehrotra told analysts.While the memory downturn remains in full force, Micron executives suggested that the company sees early signs of demand recovery.Mehrotra said that customer inventory improvements have progressed largely in line with Micron’s expectations in most markets, reinforcing management’s confidence that DRAM bit demand will return to healthy year-over-year growth in the second half of 2019. NAND bit demand is also increasing in most markets, Mehrotra said.For the fiscal third quarter, which closed May 30, Micron reported sales of $4.79 billion, down 18% from the fiscal second quarter and down 39% compared to the third quarter of fiscal 2018.Micron reported a fiscal third-quarter net income of $840 million, down 48% from the previous quarter and down 78% compared with the year-ago quarter.Micron’s DRAM revenue for the quarter was roughly $3 billion, down 5% year over year and 19% sequentially from the fiscal second quarter. NAND revenue declined to about $1.5 billion, down 25% year over year and down 18% sequentially, Micron said. The company said that DRAM and NAND sales were both hurt significantly by restrictions in sales to Huawei.For the fiscal fourth quarter, which closes in August, Micron said that it expects sales to be between $4.3 billion and $4.7 billion, below consensus analysts’ expectations of about $4.9 billion, according to Yahoo Finance. "
Building a Practical AI Medical Imaging SystemSally Ward-Foxton,https://www.eetimes.com/document.asp?doc_id=1334864,https://m.eet.com/media/1312406/KCLNvidiaAIhandshake-800x4501.jpg,Medical Designline,06.26.19,"LONDON — A new project is set to build the first AI platform for medical imaging, to be used by the UK’s National Health Service (NHS). The project, which will be run by King’s College London (KCL) in collaboration with Nvidia, aims to automate the interpretation of radiology data, the most time-consuming parts of the radiology process.While AI and machine learning have long been touted as an enabling technology for medical imaging analysis and diagnostics, implementing such a system in the real world is significantly harder than the rapid pace of technology advances would suggest.“A key challenge will be perception, within the clinical community and the public,” said M. Jorge Cardoso, CTO of the London Medical Imaging and Artificial Intelligence Centre for Value-Based Healthcare at KCL.The AI system will not make patient care decisions, but will support clinical teams to ensure the right patients are prioritized, and provide supporting information to assist in decision-making.“It is our responsibility to ensure that this [system] is seen as making radiologists more effective, supporting their decision-making process and ultimately improving patient care and patient throughput through the hospital,” Cardoso said.  Professor Sebastien Ourselin, Head of the School of Biomedical Engineering and Imaging Sciences, KCL with Jaap Zuiderveld, vice president for EMEA at Nvidia. (Source: KCL) Professor Sebastien Ourselin, Head of the School of Biomedical Engineering and Imaging Sciences, KCL with Jaap Zuiderveld, vice president for EMEA at Nvidia. (Source: KCL)Patient Confidentiality Another big challenge of building a system like this is the need for patient confidentiality. For the first time, the NHS will use federated learning techniques to tackle patient privacy and data governance issues.Patient ConfidentialityFederated learning ""outright avoids many of the classical issues of privacy in healthcare,” said Cardoso. “The key goal is to not move the clinical data from where it currently rests — this means that it stays within the hospitals’ protection, under its governance and security procedures.”While most AI systems centralize data from end devices to train one central model in the cloud, federated learning lets end devices download the model, train the model locally using locally available data, then summarize the changes and send that data to update the central model. This means individual NHS trusts can develop their own algorithms without the data travelling outside of their domain. Trusts’ separate models can be combined to build an overall model, which has effectively been trained on all demographics.“Architecting a federated [learning] solution on this scale is challenging as you need to ensure that all these large technical and clinical systems interact correctly so that all the expert teams from all the consortiums can work together successfully,” Cardoso said.Cardoso explained that there are also many ethical and governance challenges to overcome, from the agreement with clinicians around the clinical pathways, to the deployment of the system in the hospital IT environment, to making sure that patients, caregivers and clinicians see AI as a benefit to the clinical workflow.Getting the algorithm ready for deployment ""will be a slow process and will require a continuous increase in evidence supporting these systems,” Cardoso said. “It’s likely that algorithms will not be deployed directly for patient diagnosis; first, they will be used as safety systems, then maybe as prioritization systems, then triaging systems, then as secondary readers, and only then as primary readers. Every one of these steps will require substantial validation and continuous development to ensure these algorithms will not harm a patient.”"
Lots of Spin Left for Hard DrivesGary Hilson,https://www.eetimes.com/document.asp?doc_id=1334865,https://m.eet.com/media/1312404/cover-carousel-800x3501.jpg,Memory Designline,06.26.19,"TORONTO – “I’m not dead yet.” The classic line from Monty Python and the Holy Grail is an apt description of the hard drive. Despite the low price of flash and the innovation around SSDs, in part due to NVMe (non-volatile memory express), there’s still a healthy market for the spinning disk.Monty Python and the Holy Grail“There are a lot of people who say the whole world is going to move to flash but, you know, I'm not one of them,” said Jim Handy, principal analyst of Objective Analysis. “There's still a very good place for hard drives in what they call ‘cheap and deep’ storage.” And as much as flash has come down in price to be less of a premium media, there’s still a 10-to-1 gap between NAND flash and hard disk drive prices, he noted. “That is not going to go away.”In the meantime, the line between storage and memory continues to blur as the number of tiers in the storage hierarchy has expanded. Once it used to be as simple as DRAM at the high end and spinning disk at low end, but the flash in the middle has grown, and Intel is hoping there’s a place for Optane in there as well. Even tape remains a viable storage option for offsite backups, and there are multiple caches in today’s processors. “There's no good reason to limit the number of tiers as long as you can use more tiers to make your system perform better at a lower cost,” said Handy.What’s changing is that hard drives are being used in a more intelligent way, thanks to software and deployment architectures — much like how SSDs are benefitting from NVMe and NVMe over Fabric. Western Digital recently announced its Zoned Storage initiative that leverages hard drives as part of a storage design aimed at cloud and hyperscale data centers. The architecture includes shingled magnetic recording (SMR) HDDs combined with the emerging Zoned Namespaces (ZNS) standard for NVMe SSDs to deliver the necessary capacity to meet zettabyte-scale storage demands as well as better endurance and predictable, low-latency QoS performance.  Western Digital’s Zoned Storage initiative groups similar data into larger chunks to organize workloads for better performance efficiencies, putting writing sequential data to SMR hard drives while also making use of advances in NVMe SSDs. (Source: Western Digital) Western Digital’s Zoned Storage initiative groups similar data into larger chunks to organize workloads for better performance efficiencies, putting writing sequential data to SMR hard drives while also making use of advances in NVMe SSDs. (Source: Western Digital)The use of SMR HDDs demonstrates that not all emerging workloads require NAND flash or even Optane, but still need something other than general-purpose architectures, according to Eddie Ramirez, senior director of product management at Western Digital. More of this data, such as video, internet of things (IoT), edge, and surveillance data are sequential in nature. The Zoned Storage initiative groups similar data into larger chunks to organize workloads for better performance efficiencies, including making more use of available storage on a hard drive as high as 20TB, he said, with the company’s 15TB Ultrastar DC HC620 SMR HDDs already shipping in volume.Ramirez said Western Digital is in a unique position in that it plays in both the SSD and hard drive markets. “We're seeing the trends from both sides. There will continue to be a place for both SSD and HDD in the data center. If you look at the amount of NAND capacity in the world, it’s not going to scale with the data growth rate.” He said there always needs to be combination of both, and that spinning disk will always cost significantly lower than flash per terabyte.What’s changed is that the “one size, one type of HDD fits all kinds” paradigm doesn’t work anymore, said Ramirez, so just as SSDs have been segmented over the past few years for different workloads, there’s a need for purpose-built HDD storage optimized for the workloads that are going to dominate the data center in the coming years. A decade ago, the data going into cloud data centers was primarily databases from ERP systems — it was very structured, he said, but today’s data is a lot different. It’s comprised of video streaming surveillance data, for example, as well as smart city device data enabled by LTE (long-term evolution). “A lot of it will be continuous. A lot of it will be large kinds of data that need to be stored very differently from the old kind of database dominated data.”"
Foxconn Founder Replaced by New ChairmanAlan Patterson,https://www.eetimes.com/document.asp?doc_id=1334859,https://m.eet.com/media/1312397/Foxconn_building-min.jpg,Internet of Things Designline,06.25.19,"Terry Gou, the founder of Foxconn and Taiwan’s wealthiest person at $7.3 billion, has resigned as chairman at the world’s biggest electronics manufacturer in order to stand for election as the president of Taiwan.Liu Young-Way, the head of Foxconn’s semiconductor unit, will succeed Gou as chairman. The company denied media reports that it plans to shift production away from China, where Foxconn employs more than a million people assembling electronic devices such as Apple’s iPhone and iPad.Earlier this month, Foxconn held a rare public event in Taiwan to discuss the future without Gou, who has been the driving force behind the company since he founded it in 1974, originally making plastic knobs for black-and-white TVs. Foxconn today is a global company that makes everything from connectors to LCDs and semiconductors. Its top customer, Apple, accounts for about half of the company’s production.One person notably absent from the new management team is Sharp President Jeng-wu Tai, according to Mizuho analyst Yasuo Nakane.Tai, a member of Gou’s inner circle, is one of the key people who has made Sharp profitable following Foxconn’s $3.5 billion acquisition of the company in 2016, according to Nakane. Sharp will remain under Gou’s overall control, a sign that Foxconn’s founder will still be a player in the electronics industry.Sharper Focus At the announcement earlier this month, the new management outlined the company’s medium and long-term business plans, which include more focus on products for IoT, AI, 5G equipment and 8K televisions. The new management team said they aim to improve margins, which have slumped in recent years as the Apple business wanes.Sharper Focus“The environment will remain tough at least through 2020,” Nakane said in a report emailed to EE Times. “The company’s direction and strategy accounts for this tough environment.” While the company has begun providing more information on products and strategy, there are still a number of questions left unanswered, Nakane said.EE TimesUp to now, the company has been notorious for its lack of transparency. As usual, Foxconn did not respond to a request from EE Times to comment in detail. EE TimesAccording to Mizuho’s Nakane, the key questions are how Gou will be involved in the company and whether the group will continue as a contract manufacturer or shift to providing services, software, or products under its own brand. In addition, it remains to be seen how the company, its consolidated subsidiaries and unconsolidated subsidiaries will be connected in terms of strategy and business development.The new team will manage subsidiaries such as Foxconn Industrial Internet, supplying Apple as well as server and communications customers; Foxconn Interconnect Technology, making connectors and automotive components; and Foxconn International Holdings, the smartphone OEM unit.However, the committee will not be involved with the overall group, which includes unconsolidated subsidiaries such as LCD makers Sharp and Innolux and smartphone operations with Nokia/HMD, which remain under the control of Terry Gou. Foxconn is making its own phones now through an investment stake in HMD, the Finnish company behind the Nokia brand of handsets. Foxconn subsidiary FIH Mobile owns 6% of HMD and makes the phones that its Finnish partner designs.Gou’s continuing control of the Sharp and Nokia-related operations highlights his long-term interest in becoming a player in electronics brands. By relinquishing control of the contract manufacturing side of Foxconn that serves Apple, Gou may be preparing to focus more on building Foxconn’s own brands in the future.Such a move would follow in the footsteps of other Taiwanese companies such as Acer and Asus, which years ago split their contract manufacturing and branded operations to avoid direct competition with customers such as Dell and HP.US-China Trade War Foxconn is one of the companies in the global supply chain that’s at risk of being caught in the crossfire of the US-China trade war.US-China Trade WarThe company said it is aware of the risks related to a prolonged trade war and is prepared to adapt to rapid changes in the business environment.It seems confident in its ability to quickly move factories based on customer needs, according to Mizuho’s Nakane. He estimates that 75% of Foxconn’s manufacturing is in China.At the announcement, the company demonstrated products and technology that the group has been working on in preparation for the shift to 5G and 8K. Such products include 8K TVs/electronic blackboards, server racks, 5G small cells (BBU+RRU) and semiconductors like CMOS sensors, LCD drivers, 8K TV SoCs and ARM-based SoCs.Foxconn aims to boost profitability by leveraging big data related to manufacturing. The company is also adopting factory automation based on its proprietary technology. Those millions of workers the company employs in China may gradually be replaced with robots made by Foxconn.  "
Aspinity Puts Neural Networks Back to AnalogJunko Yoshida,https://www.eetimes.com/document.asp?doc_id=1334857,https://m.eet.com/media/1312387/PutNNbacktoAnalog-min.jpg,Internet of Things Designline,06.25.19,"Aspinity, a Pittsburg-based startup founded in 2015, is launching Tuesday a Reconfigurable Analog Modular Processor platform, or RAMP. The ultra-low power, analog processing platform is designed to first detect, analyze and classify raw sensor data — in the analog domain. Once it distinguishes data (a voice, an alarm, a change in vibrational frequency or magnitude, etc.) from background noise, RAMP hands off the data for digitization.The upshot of this “analyze-first-in-analog” approach is that it “reduces the power required at the edge by up to 10x and the volume of data handled by up to 100x for always-on applications,” according to Aspinity. The startup claims that RAMP can play a key role in battery-operated, always-on sensing devices for consumer, smart home, IoT and industrial markets.Mike Demler, senior analyst at The Linley Group told EE Times, “RAMP’s most distinctive feature is its extreme low power. Drawing just 10 microamps during active operation is quite a feat for an analog chip.”EE TimesAspinity’s founder and CEO Tom Doyle told us that he was delighted when he recently heard Gene Frantz talking about “the need to move neural networks back to analog.” Frantz, previously a technology fellow and a staunch promoter of DSP at TI, is now a professor at Rich University. Earlier this year, in an interview with EE Times, he suggested that AI needs a better solution and for that, “we should consider going back to analog signal processing.”This was music to Doyle’s ears. Analog processing is precisely what Aspinity’s RAMP is set up to do.Analog vs. Digital Other chip vendors including STMicroelectronics and Renesas, for example, have been pitching end-point devices featuring AI capabilities for anomaly detections. How does Aspinity’s RAMP differ?Analog vs. DigitalJoe Hoffman, director of wireless connectivity & machine sensing at SAR Insight & Consulting, said, “STMicroelectronics and Renesas utilize digital technology. They implement the fundamental elements of the artificial neuron by using digital circuitry and software on their core processors.” ST, for example, depends on ARM microprocessor cores, while Renesas uses its own Dynamically Reconfigurable Processor (DRP) — which Hoffman described as “a hybrid approach somewhere between and microprocessor core and an FPGA.” He said, “The DRP can be reconfigured on the fly.”In contrast, Aspinity’s RAMP uses an analog circuit approach. Hoffman noted, Aspinity builds neurons and synapses by using analog designs rather than digital designs.As a result, instead of developing a predictive maintenance system that continuously digitizes thousands of points of data to monitor trends in the changes of certain spectral peaks, “RAMP can sample and select only the most important data points, compressing the quantity of vibration data by 100x and dramatically decreasing the amount of data collected and transmitted for analysis,” according to the company. Reducing the amount of data is the key to enabling a battery-operated, wireless sensor system. Mythic vs. Aspinity Analog was the original way to model neural networks. Digital came later, said Demler. “But more recently, researchers (and companies like Mythic and Syntiant) are looking at in-memory analog computation to reduce power compared to digital inference engines.”Mythic vs. AspinityBy eliminating the digital memory transactions required in a typical inference engine, “You can potentially save lots of power and die area,” Demler explained.Aspinity’s CEO Doyle said, “Just like a traditional digital computer architecture, Aspinity has both ‘code memory’ to store the structure of the algorithm and the parameters/coefficients of the algorithm, and ‘data memory’ to store a history of a signal’s characteristics as we process it. However, unlike a traditional computer, Aspinity is not using a chunk of memory blocks. Instead, both code memory and data memory “are intermixed with the compute components for efficiency and compactness,” explained Brandon Rumberg, Aspinity’s CTO and founder. Integrated inside RAMP is non-volatile memory.In a way, Mythic and Aspinity are similar because their approach is “internal analog computation.” But that’s where the similarity ends.Mythic depends on digital input. Aspinity, in contrast, processes analog inputs. Demler explained, “Mythic is just using flash memory cells as voltage-variable conductance elements to replace digital multiply-accumulators (MACs).” On the other hand, “Aspinity uses a variety of parametrized analog circuits; amps, filters, adders/subtractors, etc.”6-8 bits precision As Hoffman explained, “Digital circuitry offers much more precision in its computations than analog, and is compatible with well-known digital design processes and CMOS technology. For example, state of the art processors are all 64-bit width today, while the analog processes mentioned here [Mythic, Aspinity and others] are generally 6- to 8 bits of precision. [However], this lower precision is good enough for many applications.”6-8 bits precisionIn summary, Hoffman noted, “Aspinity is focusing on a limited application set of acoustic processing for wake word/sound detection at ultra-low power. This is advantageous when the rest of the device can be put to sleep in a low power mode.” Demler also believes that analog has its limitations, in process/voltage/temperature variability, etc. He noted, “That’s why we haven’t seen it gain much traction in commercial products.” On the flip side, though, “if you can eliminate all the digital memory transactions required in a typical inference engine, you can potentially save lots of power and die area.”Applications Aspinity sees a growing market for “voice-first devices” such as smart speakers and wearables/hearables. Doyle said, “Imagine a voice-first TV remote that runs for a year per battery change. That will give manufacturers a big competitive edge.”ApplicationsAccording to Aspinity, the RAMP platform’s analog blocks can be reprogrammed with application-specific algorithms. RAMP can analyze raw analog data from different types of sensors including accelerometers used for industrial vibration monitoring.Demler noted, “RAMP is a special-purpose circuit.” In using RAMP, designers must consider cost vs. benefits of adding another component to their voice-activated devices. But is that a downside? Not exactly, Demler said. “RAMP is a voice (or sound) activity detector. It doesn’t determine exactly what is being said. In some systems, it would make a lot sense to integrate RAMP as the front end of a speech processor, rather than as a separate chip.”Aspinity's CEO Doyle said that he plans to be in IP business in addition to selling chips. The company currently has a number of partners it's working with. ""Some are cosumer companies and others are chipset partners,"" said Doyle. The chip is sampling today. The plan is to go into volume production in the first half of 2020.Company  Aspinity was founded to commercialize research at West Virginia University. The startup has exclusive, full rights to use the technology developed at the university.Company Aspinity has raised a total of “$3.6 to $3.7 million” in funding over three rounds. According to the CEO, Amazon participated in two rounds. The company has a team of ten engineers, many with extensive analog experience, said Doyle. — Junko Yoshida, Global Co-Editor-In-Chief, AspenCore Media, Chief International Correspondent, EE Times EE Times"
Renesas Replaces CEO in Bid for Growth TurnaroundNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334861,https://m.eet.com/media/1312400/HidetoshiShibataRenesasCEO.jpg,,06.25.19,"Renesas Electronics Corporation has appointed Hidetoshi Shibata as its new president and CEO, replacing current CEO Bunsei Kure, forced to resign as a result of not achieving performance targets.In a statement, Renesas said its nominations committee, a voluntary advisory body reporting to its board of directors, evaluated Kure’s performance and decided a change of management was necessary in order to recover its business performance and return the company to a growth trajectory. It said in light of its worsening business performance and a huge gap between targeted financial figures which were set as the mid-term plan in 2016 and the company’s actual performance, it came to the conclusion that Kure did not meet its expectations, especially in view of Renesas’ current situation and as compared with trends at competitors.Hence the board decided to appoint its current CFO, Hidetoshi Shibata, as the new president and CEO. Shibata joined Renesas in 2013 when the company was in crisis, and as executive vice president and CFO, led structural reform measures, such as personnel reductions, and the reorganization of production sites. He was instrumental in leading the acquisitions of two semiconductor companies, Intersil Corp. and Integrated Device Technology, Inc., which the company said are critical for the company’s future growth.In many ways, Shibata is a man who knows Renesas inside out. Prior to joining Renesas, Shibata was executive managing director at the Innovation Network Corporation of Japan (INCJ) -- a public-private partnership between the Japanese government and major corporations. While at INCJ, he led a team to invest and rescue Renesas.Renesas is clearly expecting Shibata's decisiveness will serve the company well as he seeks to improve business performance and consequently the company's stock performance.Shibata commented, “Whilst we face short-term challenges that require concerted efforts across the company, I believe that Renesas is well placed in our focus markets to continue to create innovative, sustainable solutions for our customers and society at large.” He added he would maintain the positive momentum of the IDT integration process."
"Patents, Standards Brew Licensing Woes Rick Merritt",https://www.eetimes.com/document.asp?doc_id=1334856,https://m.eet.com/media/1312379/PatentMPEG-min.png,Wireless and Networking Designline,06.24.19,"SAN JOSE, Calif. — The latest codec for compressing video isn’t getting market traction because costs of licensing patents for it are unclear. That’s motivated a handful of efforts to fix decades-old problems licensing standards-essential patents that have dogged everything from cellular to Wi-Fi.With some proposed approaches, project members agree on licensing terms before work begins rather than leaving the job to lawyers sometimes years after a standard is set. Others have refined definitions of fair, reasonable, and non-discriminatory (FRAND) licensing, an often-used concept that has proved to be dangerously vague.Still others advocate a radical move to royalty-free standards. So far, this approach has not been immune from attacks from patent holders, nor has it been sufficiently motivating to attract some top technology developers, especially companies with significant licensing businesses.There is yet no single method that guarantees a worthwhile return for developing new technologies while preventing abuse from opportunists and monopolies, given the industry’s diversity in business models, technologies, and markets. And some see in standards-essential patents (SEPs) a Gordian knot that cannot be untied or cut.“With the combination of the limited monopoly granted in a patent, and the de facto monopoly of a standard, the normal competitive forces that would determine a reasonable royalty don’t apply,” said Cliff Reader, an expert in intellectual property issues for video codecs who sees little hope that any country will enact the kind of patent reforms needed.“Patents and standards are the most challenging things to put together in tech,” said Karen Bartelson, former head of the IEEE who learned important lessons about SEPs in a patent “nightmare” over Wi-Fi.Today, SEPs are key to the high-profile Apple/Qualcomm dispute over cellular, and they are starting to impact car makers, too. SEPs are the hot spot for H.265, aka HEVC, the codec central to many network operator’s plans to deliver ultra-high-definition content.Patent wars in video codecs have raged off and on for years. It’s a field known for its deep base of constantly evolving techniques critical for broadcasters, TVs and set-top makers, and, more recently, video streaming services and smartphones.Tensions reached a peak when three patent pools emerged for HEVC. One of them, Velos Media, has not yet disclosed its licensing terms, and more than a dozen large patent holders have not joined any of the pools.“That’s a complete nightmare to navigate, and it’s pretty disappointing, because we founded our company in 2012 anticipating [that] H.265 would become dominant,” said Oliver Gunasekara, chief executive of NGCodec, a video codec developer.The uncertainty has kept the previous codec, H.264, dominant with an estimated 50% market share. The newer H.265 has less than 20% of the market despite the fact that it has been available for more than six years and is technically superior — it can cut bit rates for H.264 in half without perceptible quality loss.The core problem, Gunasekara said, is that the MPEG group behind video codec standards focuses solely on technology, leaving others to sort out patent licensing terms.“We can do 1,000:1 video compression live, and you can’t tell the difference — that’s mind-blowing. MPEG creates video standards that matter. You can’t fault them technically, but they fall down by relying on FRAND.”The crisis got so bad that MPEG’s veteran leader, Leonardo Chiariglione, wrote a blog in January lamenting that “the old MPEG business model is now broke.” He expressed fears that royalty-free alternatives such as the AV1 standard started by Google will take over.Next page: A royalty-free option still attracts patent assertionsNext page: A royalty-free option still attracts patent assertions"
AI Gets Inference BenchmarksRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334852,https://m.eet.com/media/1312373/MLPerflogoinvert-min.png,AI & Big Data Designline,06.24.19,"SAN JOSE, Calif. – The MLPerf consortium released benchmarks to measure inference tasks on servers and client systems. Tools to run MLPerf Inference v0.5 are available on the group’s Web site, but vendors are not expected to start posting results using the metrics until October.Initially, MLPerf Inference includes five benchmarks to measure performance and power efficiency of tasks run on a smartphone, PC, server and some embedded systems.  They consist of two tests for image classification, two for object detection and one machine translation, each with a defined model and data set.The metrics target “many, but not all embedded systems,” said David Kanter, the inference group’s co-chair.  “Our load generator is written in C++ (instead of Python), which enables running on much more resource-constrained systems,” he said.“However, there are probably some platforms that are so small or specialized for cost/power reasons that they will not be able to run our load generator or our networks. For example, we don't have a suitable benchmark for a system which only does wake-word detection,” Kanter added.The benchmarks will measure average active power consumed during inference and active idle power when the system is waiting for inference queries. It includes several metrics to describe raw performance specific to different scenarios along with rules for applying them.For example, for a single stream it measures responsiveness based on 90th percentile latency to answer a query. It can also report the number of simultaneous inference streams a system can sustain. Two other metrics measure throughput for online and offline batch systems.So far, 17 organizations expressed interest in submitting inference results on a total of 196 combinations of different models and scenarios that range from a single stream on a handset to a batch job on a server. The vision benchmarks use the ImageNet dataset and MS-COCO widely used in robotics, automation, and automotive. The translation test leverages an existing English-German benchmark.MLPerf provides both the benchmarks specs and reference code implementations in ONNX, PyTorch, and TensorFlow frameworks to run them. The code defines the problems, models, and quality targets, and provide instructions to run the benchmarks.The specs and tools were developed over 11 months by the group that included members from Arm, Cadence, Centaur Technology, Facebook, Futurewei, General Motors, Google, Habana Labs, Harvard University, Intel, MediaTek, Microsoft, Nvidia, and Xilinx. So far, the group has “had very minimal discussions about the v0.6 version,” that will form its next step, said Vijay Janapa Reddi, an associate professor at Harvard who is the inference group’s other co-chair.“We downsized the original batch of models we had in mind to meet the current v0.5 timeline,” said Reddi. “In the v0.6 version, we will have more models and likely update some of the models for the existing tasks,” he said.“Other questions we always ask ourselves is ‘How can we get a larger universe of submitters,’ and ‘How can we make submitting easier,’” said Kanter, adding that future versions may also be capable of running on more constrained embedded systems.MLPerf is a collaboration among more than 40 companies and universities. Last year, the group rolled out its Training v0.5 benchmark suite. So far, Google, Intel and Nvidia released scores of their chips using it.Related posts:Related posts:"
Top 10 Design Outsource ServicesCabe Atwell,https://www.eetimes.com/document.asp?doc_id=1334838,https://m.eet.com/media/1312291/caro.jpg,Prototyping Designline,06.24.19,"(Source: Pixabay)The time when electronics companies’ product production cycle, in-house circuit design and development, manufacturing, testing, and packaging was all done under the companies’ moniker may be coming to an end. The onset of cheap components and labor quality has allowed companies, startups, and even makers to outsource their electronic design work to offshore entities that provide services which help them develop electronic products from start to finish. You bring the idea — others can make it a reality.No matter if you’re looking to build a new smart device, IoT platform, or a new single board computer, there are myriad companies that provide services which can take your ideas and make them a reality. And there are varying outsourcing service companies that can handle the product life cycle from concept to finish. However, some cater only to large- and medium-sized businesses while others can entertain the little guy or even makers. In this roundup, we will look at some of the better outsourcing services on the market for every business tier and what they have to offer.Next page: Titoma Electronic Product Design for China Manufacturing  Next page: Titoma Electronic Product Design for China Manufacturing  "
You Say Your AV Is Safe? Show MeJunko Yoshida,https://www.eetimes.com/document.asp?doc_id=1334851,https://m.eet.com/media/1312372/safetycase.jpg,Automotive Designline,06.24.19,"Arguing credibly about the safety of any autonomous vehicle (AV) requires somewhat more proof than a company like Waymo, Uber or GM declaring that its robocars are safe for commercial deployment. The automaker must be able to demonstrate that its AI-driven vehicles meet specific and rigorous standards.The next questions then become:There are no easy answers. Further, safety standards, especially for AI-driven vehicles, come with additional twists and numerous caveats.Put simply, AI-based autonomous vehicles are running on machine-learning algorithms housed in black boxes. The inner workings are probabilistic in nature, and it’s almost impossible to determine why they make whatever decisions they make.This being so, what strategies do tech companies and car OEMs use to verify their AVs’ safety?Recommended UL Takes Autonomy Standards Plunge Recommended UL Takes Autonomy Standards Plunge UL 4600 draft standard Against this backdrop, Underwriters Labs, currently developing a “Standard for Safety for the Evaluation of Autonomous Products” — UL 4600 — said the members on its Standards Technical Panel (STP) met in person for the first time on June 12 and 13, to review and discuss the initial draft standard.UL 4600 draft standardEE Times last week caught up with Phil Koopman, co-founder & CTO at Edge Case Research, a principal technical contributor to the draft.EE TimesThe minutes of the first meeting are yet to be made public. Koopman, however, described the first meeting as “very positive and constructive."" He said the members hit all the main issues of UL 4600 that must find solutions.Who are on the roster? According to the UL website, the UL 4600 group lists some 30 or so STP members with voting rights. They include four chip vendors — Nvidia, Renesas, Intel and Infineon — and commercial AV users and developers, among them General Motors, Uber, Nio, Bosch, Argo AI and Aurora. Both the U.S. Department of Transportation (DoT) and Pennsylvania DoT are sending representatives.Who are on the roster?Interestingly, the UL 4600 STP also includes three insurance companies: AXA, Liberty Mutual and Munich Re America.In pursuit of transparency and broader inclusion, the UL 4600 group is seeking any responsible party willing to be registered as a “stakeholder.” Once registered and approved, stakeholders can request to review and comment on the draft standard. While stakeholders have no voting rights, adding them is important, explained Koopman, to ensure “a very open procedure” and to signal that AV safety “is a matter of public policy.”UL 4600 vs. ISO 26262 and ISO/PAS 21448 (SOTIF) UL 4600 is a relative newcomer to automotive safety standards development. ISO 26262 already exists, while ISO/PAS 21448 (safety of the intended functionality or “SOTIF”) are well into development. The most frequently asked question about UL 4600 is why the automotive world needs yet another standard?UL 4600 vs. ISO 26262 and ISO/PAS 21448 (SOTIF)Stressing that the UL 4600 group is closely in touch with leaders in ISO 26262 and ISO/PAS 21488, Koopman made it clear that “resolving potential overlap is an ongoing activity.”Developing safety standards based on the assumption that systems will have no responsible human driver is what separates UL 4600 from other standards.In contrast, “existing standards such as ISO 26262 and ISO/PAS 21448 were envisioned for vehicles that ultimately have a human driver responsible for safe operation of the vehicle,” Koopman noted. In his opinion, the technology in robocars and other autonomous systems “exceeds the scope of these and other traditional safety standards. Those standards are necessary, but not sufficient.”Did you think of that? In other words, in developing self-driving cars, Koopman believes automotive design engineers will soon discover numerous issues they hadn’t even thought about before. Speaking of “the pervasive implications of vehicles not having a responsible human driver,” Koopman put those items under the rubric of “Did you think of that?”Did you think of that?Expect the UL 4600 to be much more prescriptive compared to other standards.While ISO 26262 and SOTIF provide safety as a “target” to shoot for, UL 4600 offers a “bullseye,” said Koopman. For instance, UL 4600 will expect from automotive designers a lot of details, such as, “if you are doing X, don’t forget to do Y.”  Other standards show “how to get to safety,” but UL 4600 prescribes “where you end up with your system.”Next page: Make the safety case Next page: Make the safety case "
Flexible RFID ICs To Tackle Counterfeit Consumer GoodsNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334853,https://m.eet.com/media/1312375/newpragmatic_flexics.jpg,Internet of Things Designline,06.24.19,"Chinese packaging materials firm BSN intends to use flexible electronics firm PragmatIC’s RFID ICs in a new printing facility focused on anti-counterfeit solutions for fast-moving consumer goods (FMCG), online sales, and pharmaceutical drugs.BSN is part of the Baoshen Group, one of China’s leading packaging material suppliers for footwear, apparel, bags, furniture, cosmetics and accessories, with an annual capacity of 7 billion print items and 1 billion RFID labels. PragmatIC’s ConnectIC products will be used in BSN’s new Eprint line of RFID products.The ConnectIC family, released earlier this year, provides connectivity solutions using HF RFID proximity identification aimed at smart packaging, and targets markets such as food and beverage, personal and home care, pharmaceutical and healthcare.The RFID elements are thin and flexible, making them suitable for embedding into a wide range of substrates, including plastic or paper. The company claims they are imperceptible when added to labels or packaging. With integrated energy harvesting and designed for use with single layer antennas, they are designed to minimize total system cost.Headquartered in Cambridge UK, PragmatIC says the low cost of these flexible ICs means electronic connectivity is no longer limited to high-value luxury items. The company contends its technology now makes it economical to add ""smart"" features such as traceability and interactivity to high-volume FMCGs and other mass market applications.The ConnectIC series facilitates rapid detection of objects when one or more low-cost custom readers are integrated into the system. Designed for proximity identification applications, the ICs can then enable applications such as hierarchical inventory management, item identification and tracking, supply chain assurance and brand authentication. Scott White, CEO of PragmatIC, said, “The ConnectIC family is set to bring connectivity to items we buy every day.”BSN’s RFID consultant Philip Calderbank commented, “We see rapidly growing demand in Asian markets for solutions to protect brands and consumers from counterfeit products both in store and online. PragmatIC’s ultra-low-cost technology will enable us to roll out solutions to a much wider range of market segments than possible with conventional electronics.”"
UltraSoc Gets $6.3M for Hardware-Level CybersecurityNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334854,https://m.eet.com/media/1312376/UltraSocbaremetalsecuritydiagram.png,Test & Measurement Designline,06.24.19,"UK-based embedded analytics firm UltraSoC has secured £5m (about US $6.3 million) in new funding to target hardware level cybersecurity, hire more hardware and software engineers in the UK, and open a data science and machine learning engineering center in Warsaw, Poland.UltraSoC said it had already established itself as a solution for semiconductor companies needing to understand how their system-on-chip (SoC) products are behaving. While this results in reduced development and debug costs, those same customers are also recognizing the potential for its technology in implementing functional safety and cybersecurity features.CEO Rupert Baines commented, “This funding round will enable us to grasp that opportunity, which I believe we are uniquely equipped to address.”The company said its approach to cybersecurity places anomaly detection directly into the silicon, enabling hardware-based threat detection. This can identify attacks very quickly – in microseconds rather than milliseconds. Also, since it is ‘below the OS’, it is therefore very hard for an attacker to detect, circumvent or subvert.Baines told EE Times, “We can efficiently build a monitoring and communications infrastructure that functions independently from the main system. So, that in itself provides an extra layer of security, over and above standard techniques: an addition to the single root of trust provided by standard systems.” He said UltraSoC enables a system to be configured that functions entirely non-intrusively, if that is needed. “Our monitors react at hardware speed, so we can respond very quickly indeed (microseconds), if an intrusion takes place.” EE TimesHe added “We can also build ‘smarts’ into the hardware — I can’t say too much about that at the moment, because it’s very definitely a highly differentiated proposition. We’ve shared these concepts with partners and security experts who assure us that what we can do is genuinely unique.”In addition to hiring engineers at its headquarters in Cambridge, UK, and design center in Bristol, UK, it also plans to open an engineering center in Warsaw, Poland to develop its data science and machine-learning technologies. Baines told us that Warsaw and Krakow have excellent strength in data science, and this will be a major focus for the design center as the company looks more into how it can help its customers and their customers to make use of the rich data it collects from chips.The Polish center will be headed up by UltraSoC’s director of system engineering Marcin Hlond, who was previously with Blu Wireless and Icera before joining UltraSoC in 2017 to head up the company’s Bristol, UK, engineering team.  Baines told us it was serendipitous that Hlond was looking to move ‘back home to Poland’ just as UltraSoC was looking to establish a presence in the European Union.With the funding, UltraSoC also aims to expand customer support and commercial presence in Europe, the USA, Japan and China. In total, Baines said it plans to double the size of its workforce by the end of 2020, from its current 35 people.UltraSoC said it has had a successful period of growth and new technology developments within the last year, both in terms of revenues and licensees. Baines said they don’t quote revenue, but the firm now has 20 licensees. He commented, “The technology is also very ‘sticky’, so we have a 100% retention rate – our customers invariably become repeat customers.” Early in 2019, it announced data storage firm Seagate had licensed its technology, and that it is supporting Western Digital’s SweRV processor architecture. It also recently announced Alibaba company C-SKY, Esperanto, Kraftway and Microchip as customers. Its core technology improvements include development of its architecture to address high-performance computing and exascale systems, a solution targeting automotive lockstep applications, and an expanded ecosystem for heterogeneous designs incorporating the RISC-V open source architecture, and new patents.UltraSoC’s latest funding brings the total amount raised to £19 million, and this round brought in cybersecurity-focused venture capital firm eCAPITAL, and Seraphim Capital, a specialist investor in the space ecosystem. They joined earlier investors Indaco Venture Partners, Octopus Ventures, Oxford Capital, Techgate, and business angel Guillaume d’Eyssautier."
iBASEt & Amazon Collaborate on Aerospace & Defense Manufacturing Platform Hailey Lynne McKeefry,https://www.eetimes.com/document.asp?doc_id=1334840,https://m.eet.com/media/1312304/iBASEt_Blog-Banner-f35.jpg,Military & Aerospace Designline,06.21.19,"Software vendor iBASEt is collaborating collaboration with AWS, as part of Amazon Web Services’ Smart Factory program, to bring a cloud-based Digital Manufacturing suite for aerospace and defense manufacturers using AWS. “iBASEt is working with AWS to bring the iBASEt Manufacturing Suite on AWS GovCloud where more stringent requirements are in place to protect the data,” Sung Kim, chief technology officer of iBASEt told EETimes.Aerospace and defense customers, who are contending with extensive regulations, can use the platform to efficiently leverage native services to create and deploy applications and perform platform updates. A platform approach allows discrete manufacturers in the aero/defense space to have better visibility and control, while lowering cost of ownership and increasing security.“By working with AWS, either on commercial or GovCloud, our clients will be able to fully leverage a cloud environment to drive greater efficiency and control over their manufacturing operations and the digital thread,” “The cloud creates a powerful infrastructure to leverage a range of native services that drive seamless adaption of different technologies, connecting operations and sustainment management in a seamless flow of data across the value chain and product lifecycle.” In addition, the platform provides enhanced platform management troubleshooting, automatic software updates and simplified implementation, the company said. “In a cloud environment, iBASEt’s manufacturing solution will leverage AWS best practices for security and high availability in an environment that also supports quick deployment using automated processes,” said Dr. Josef Waltl, global segment lead for Industrial Software at Amazon Web Services. Amazon Smart Factory provide cloud-based infrastructure to organization’s working toward factory automation.  “AWS brings the best practices of putting enterprise applications in cloud, which allows for greater efficiency for manufacturers,” said Kim. “iBASEt is leveraging native AWS services and utilizing them to provide higher levels of performance and scalability.” The AWS service includes AWS Cloud IoT Services, edge computing, data lakes, and advanced analytics tools to improve manufacturing operations by capturing, harmonizing, analyzing, visualizing, and executing on silo'd plant floor data, leading to improvements across critical KPIs such as quality, yield and Overall Equipment Effectiveness (OEE), according to Amazon.  AWS also helps manufacturers leverage AI and Machine Learning for real time and predictive analytics capabilities.Amazon’s AWS Quick Start program can be used with the iBASEt Manufacturing Suite to simplify deployment as customers trying to move to the cloud. “Unlike other solutions, iBASEt provides manufacturers with a platform to deploy a variety of applications and manage regular platform updates more efficiently, which enables discrete manufacturers in the A&D market to have better visibility and control of their operations and simplified implementation in a complex, highly regulated industry,” said Kim.The digital manufacturing suite is being used by some well-known names in the aerospace and defense customers.  Last April, iBASEt announced contract agreement with aerospace company Lockheed Martin, which is using the platform as its next-generation manufacturing execution system (MES) for its Aeronautics division, which designs and manufactures military aircraft.   "
Volvo Group to use Nvidia AI for Autonomous TrucksNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334847,https://m.eet.com/media/1312328/190618-Volvo-NVIDIA-7.jpg,Automotive Designline,06.21.19,"Gothenburg, Sweden — As the industry hype over autonomous vehicles moves into the realms of reality, or what’s really possible in the near term, one area where it could really have great immediate impact is in the freight industry. Recognizing this, Volvo Group and Nvidia have partnered to use the latter’s artificial intelligence (AI) platform to deploy autonomous trucks.Volvo Group announced this week it is using Nvidia’s Drive end-to-end autonomous driving platform to train, test and deploy self-driving AI vehicles, targeting public transport, freight transport, refuse and recycling collection, construction, mining and forestry.They will co-locate engineering teams in Gothenburg and Silicon Valley to build on the Drive AGX Pegasus platform for in-vehicle AI computing and utilize the full Drive AV software stack for 360-degree sensor processing, perception, map localization and path planning. They will also test and validate these systems using the Nvidia Drive hardware-in-the-loop simulation platform.Jensen Huang, Nvidia founder and CEO, said the Volvo Group partnership was a landmark for the future of transportation. Together with Volvo Group CEO Martin Lundstedt, they said AV technology applied to an entire lineup of trucks operating around the world can bring enormous benefits for industries ranging from public and freight transport to forestry and construction, to become more efficient, with vehicles that can work longer and travel farther.Online shopping and transport  Autonomous trucks address the demands of today’s online shopping, which put huge stress on transport systems — especially with expectations for overnight or same-day deliveries. More than 35 million packages worldwide are delivered each day, and this is growing up to 28% percent annually. By 2040, delivery services will have to travel another 78 billion miles each year to handle goods ordered online, according to consultancy firm KPMG.Online shopping and transport Autonomous trucks can potentially help meet this demand. They can operate 24 hours a day, improving delivery times, and with increased efficiency, can bring down the annual cost of logistics in the U.S. by 45% — between $85 billion and $125 billion, according to experts at McKinsey.From automating short, routine trips like the loading and unloading of containers on cargo ships and managing port operations, to autonomously driving on the highway, Volvo’s new generation of vehicles could help streamline the shipping industry. By using the Nvidia Drive solutions within the second-largest truck maker globally, Nvidia and Volvo Group say they can bring the efficiencies of autonomous trucking to the transport industry.Volvo Group will be utilizing Nvidia Drive Constellation to test and validate AVs, ensuring they can handle diverse operating challenges all over the world. By utilizing hardware-in-the-loop simulation, the companies can test the autonomous driving systems on the same hardware and software that will run in the vehicle, at a significantly greater scale."
Facial Recognition: The Ugly TruthSally Ward-Foxton,https://www.eetimes.com/document.asp?doc_id=1334831,https://m.eet.com/media/1312257/Trueface-800x6001.jpg,AI & Big Data Designline,06.20.19,"The UK is one of the most surveilled countries in the world, with closed-circuit televisions (CCTV) cameras everywhere, from shops to buses to private homes. In the last couple of years, AI techniques, such as neural networks, have propelled large-scale automated facial recognition. Combined with the UK’s huge network of existing CCTV cameras, this could hold vast potential for security applications.Our technological capabilities are developing faster than are the laws that dictate how they can be used. There are also many gray areas that require a lot of deliberation. With this set of articles we examined the technology and what we can do with it, and consider what we should do with it.Our technological capabilities are developing faster than are the laws that dictate how they can be used. There are also many gray areas that require a lot of deliberation. With this set of articles we examined the technology and what we can do with it, and consider what wedo with it.  However, there are worrying implications for privacy. Biometric facial data taken from photos or CCTV footage is particularly troubling because it can be taken without the person’s consent and without them knowing about it. This data may be collected indiscriminately — whether the subject is matched with a watch list or not — and it allows people to be located and for their movements to be tracked. It’s easy to imagine photos of crowds with every face linked to the person’s identity, then potentially connected to all kinds of other data about them. A promotional picture for NEC’s NeoFace facial recognition technology. (Source: NEC) A promotional picture for NEC’s NeoFace facial recognition technology. (Source: NEC)Law Enforcement  If we are going to trade our privacy for increased security, the benefits must outweigh the costs. Is facial recognition technology actually effective in catching criminals?Law Enforcement Police forces around the UK have performed trials of live facial recognition technology in public places such as shopping malls, sports stadiums, and busy streets. The Metropolitan Police have undertaken 10 trials of the technology so far, using dedicated camera equipment along with NEC’s NeoFace algorithm, which measures the structure of each face, including distance between the eyes, nose, mouth, and jaw. According to NEC, this algorithm can tolerate poor quality image such as compressed surveillance video, with the ability to match images with resolutions as low as 24 pixels between the eyes.Despite winning four consecutive NIST FIVE (U.S. National Institute of Standards and Technology, Face in Video Evaluation) awards for performance, NeoFace didn’t do terribly well in the field. In the eight deployments between 2016 and 2018, 96% of the Met’s facial recognition matches misidentified innocent members of the public as being on the watch list.A major criticism of the trials has been that the public were not sufficiently alerted to them and were not given the opportunity to opt out. The BBC filmed one passerby in North London covering his face — the police photographed him anyway and gave him a £90 fine for disorderly conduct. A South Wales Police van with facial recognition cameras. Text on the side of the van reads “Facial recognition fitted” in English and Welsh, but vans used by the Metropolitan Police were less clearly marked. (Source: Liberty) A South Wales Police van with facial recognition cameras. Text on the side of the van reads “Facial recognition fitted” in English and Welsh, but vans used by the Metropolitan Police were less clearly marked. (Source: Liberty)South Wales Police’s more extensive trials of the technology were marginally more successful: only 91% of matches were misidentifications of innocent members of the public. Images of all passersby, matched or not, were stored by the police for 31 days.This trial prompted one member of the public to take legal action against South Wales Police, having been photographed while out shopping and while at a peaceful anti-arms protest. He claims that automated facial recognition technology violated his right to privacy, interfered with his right to protest, and breached data protection laws (judgment is due this fall)."
"Living on the Edge: MEMS, Sensors and Cybersecurity Maria Vetrano",https://www.eetimes.com/document.asp?doc_id=1334832,https://m.eet.com/media/1312259/ABCCAROANDIMAGE1.jpg,Internet of Things Designline,06.20.19,"Though sensors and MEMS are exceedingly vulnerable to malicious hacking, it is possible to harden them against attack.  Over the past decade, cyber-savvy thieves have used hacked IoT devices in homes for pre-theft reconnaissance. Security researchers have demonstrated the ability to control vehicles by hacking on-board sensors.Infrastructure is also at risk. In 2012 Iranian hackers compromised control systems of a dam in upstate NY, and the 2018 gas-line explosions in the Boston area — while not cyberattacks — were caused by the kind of pressure-sensor errors that a hacker could emulate.While there are a multitude of ways to hack into any given system, MEMS (micro electro-mechanical systems) and sensors are especially vulnerable to attack. The attributes that make them so cheap and powerful also make them difficult to secure: they are tiny, have limited processing power, and offer little physical room for memory or additional computing.And they're ubiquitous. Mitre Corp.’s principal cyber security engineer Cynthia Wright explained, “In homes and offices, we find MEMS and sensors in air-quality alert and fire-detection systems. In cars and trucks, they enable vehicle airbag-crash sensors, tire pressure monitoring systems, collision avoidance and a multitude of other safety functions. In medical wearables, they monitor blood glucose levels, heartrate and other biological functions. We use them for insulin delivery and for neural, cochlear and retinal implants. They are integral to prosthetic arms and limbs.“MEMS and sensors also play a role in critical infrastructure such as power plants and transportation systems,” said Wright, a retired military officer with over 25 years of experience in national security and cyber strategy and policy. She added that they are widely deployed in military applications such as battlefield robots and exoskeletons, bomb bots for defusing ordinance, and laser tuning for communications and satellites.Although these MEMS- and sensors-enabled systems are incredibly diverse, they have one thing in common: their wireless connectivity — the conduit through which they become vulnerable. Despite these challenges of connectivity and capacity, the MEMS and sensors industry can improve the cybersecurity of connected electronics.Carmelo Sansone, director, SEMI-MEMS & Sensors Industry Group (MSIG), sees this challenge as an opportunity for MEMS and sensors suppliers. “Suppliers don’t need to reinvent themselves to secure wirelessly connected electronic products,” said Sansone. “They do need to fearlessly explore potential vulnerabilities in order to address them, and they need to decide which types of devices warrant added security.” Understand Vulnerabilities  Sansone advises that manufacturers must first understand the potential points of vulnerability of their products. As devices that enable interaction with the environment, MEMS/sensors generally reside at the system edge. They are often used in systems that provide physical asset security, such as motion, fire or other hazard detectors, or in security cameras. They also provide the data used by higher-level system software to make operational decisions, such as opening a valve or lowering a diverter. Understand Vulnerabilities Wright noted that because MEMS/sensors tend to be deeply embedded in a system, the data they collect is often either personally, proprietarily or operationally sensitive. “These characteristics, combined with the simplicity and low-level functionality required for such ubiquitous sensors, make them both very attractive and quite vulnerable to hackers,” she said.Wright offered an example. “In an IoT product such as a medical device or vehicle proximity sensor, MEMS and sensors gather sensor data that is essential for the device to function properly,” she offered. “A hacker might alter that sensor data to communicate erroneous information to IoT software, which, in turn, alters system performance — sometimes disastrously.”Which end-user applications are most vulnerable? According to John Chong, vice president, product & business development, Kionix, “The most vulnerable devices are unprotected network-connected devices with single points of failure. The most important to secure are those in which a compromised sensor puts the safety and security of items of value — including human lives — at risk.”Tzeno Galchev, product marketing manager, MEMS Technology Group, Analog Devices, agrees with Chong that the key question isn’t which types of connected devices are most vulnerable but rather how they are used.“It is hardest to secure MEMS-enabled connected devices in the consumer space because the threat surface is generally more expansive,” said Galchev. “As there is no physical restriction on who handles a smartphone or a wearable, for example, implementing additional protection at the device level can make it prohibitively expensive or degrade the user experience, deterring user adoption. In markets such as automotive, public infrastructure or military/aerospace, on the other hand, designers recognize the criticality of security, so they take extra care in building security into their systems. To do otherwise could result in catastrophic and life-threatening implications on a mass scale. That’s why the MEMS-enabled connected devices embedded into such systems are required to support system-wide security.” Not All Attacks Are Equal Some security attacks on sensors are more sophisticated — and potentially damaging — than others. According to Wayne Chavez, business development manager, IoT Sensors, NXP Semiconductors, “Some attacks simply ‘defeat’ the sensor entirely, rendering it inoperable. Other more elaborate attacks can replace sensor data with errant information, potentially manipulating functions, commands or the system as a whole.” Not All Attacks Are Equal Wright pointed out that shutting down a proximity sensor in an automatic garage door versus altering sensor data in a train to cause a collision are attacks of very different magnitude.There is also a difference between isolated cyberattacks and attacks with cascading effects. “One cyberattack might cause a piece of factory equipment to malfunction, which ruins the equipment,” said Wright. “Another attack might cause a refinery’s systems to fail, which then causes an explosion with a toxic plume.”Perhaps industry will offer varied levels of assured security and privacy for different applications. One level of security could suffice for smart lightbulbs, for example, while another is established for biomedical devices, automobiles or infrastructure such as dams and refineries. Who’s Responsible?  Scores of MEMS and sensors suppliers. Dozens of industry standards bodies and government regulators. Thousands of original equipment manufacturers (OEMs) and system integrators. While all have skin in the game when it comes to securing devices from cyberattack, it is unclear who should take responsibility for addressing this highly complex issue. Who’s Responsible? While government regulation might provide some long-term solutions, government is too grid-locked to offer meaningful regulation soon. Industry has the most to gain from getting ahead of this problem, and MEMS/sensors suppliers in particular can outshine the competition by offering more differentiated secure products.Chavez suggests that we to look to automotive for an example that works. “Years ago, the automotive industry united to address safety integrity levels, or ASIL,” he said. Learning from an industry steeped in safety-critical electronics sounds like good advice for MEMS/sensors suppliers.Galchev agrees that MEMS/sensors suppliers should consider best hardware security practices developed for other technologies or industries. “We should explore emerging IoT security standards, such as ISA 62443 for industrial, ETSI TS 103645 for consumer, and ISO/SAE 21434 for automotive, when we consider security standards for MEMS and sensors,” he said. “While I am less optimistic about industry-wide standardization, I am more hopeful that we can reach consensus by sensor type or use-case.”Sansone observed that setting security standards for sensors is highly complex because sensors do not share the same common classes of semiconductors such as power products, converters or microcontrollers. Galchev agreed, noting that “the sensor industry is anything but homogenous. Different types of sensors use different sense mechanisms, have different cross-sensitivities to other physical phenomenon, and generally have different security threats.”After recognizing the inherent vulnerabilities in MEMS/sensors, agreeing that we need varied levels of security for different classes of products, and acknowledging that we cannot actually determine who is responsible for fixing the cybersecurity risks associated with these intelligent devices, let’s look at something very practical and positive: how to reduce the risks associated with such devices. How Do We Fix It?  When I asked SEMI-MSIG experts how to increase the security of MEMS/sensors, they offered helpful pragmatic approaches. Galchev suggested that reducing the number of hardware ports to the device would limit the number of avenues through which physical threats can occur. At the same time, he recommended increasing software-defined functionality to add market differentiation. How Do We Fix It? “The right approach requires a balance in design, development and deployment,” said Galchev. “We need to design functionality that is market-competitive as we think holistically about the possible threats early-on during design, and then determine what needs to be mitigated while we consider any residual risks. We also close off any debug ports that are not required during device deployment.” Wright loves this design philosophy, which aligns with MITRE’s simple cybersecurity precept: “Build it in. Don’t bolt it on.” Chavez also agrees, stressing that security cannot be an afterthought, adding that his company practices a “secure-by-design” philosophy that extends from the platform level to hardware-based root-of-trust.According to Chong, ensuring that the data in motion has not been tampered with is itself a form of authentication. “Securing the data between points of encryption/decryption is one of the easier things to implement,” he said. “While it does not address other vectors of attack, such as spoofing the sensor, physically destroying the sensor, or compromising the data outside the encryption/decryption points, it is still a valuable security measure.”Galchev advocates for balancing cybersecurity approaches against the common system requirements of all edge-nodes, which include energy budget, computational capability, network and communication limitations, and cost. Like Chong, Galchev recommends establishing the level of data integrity at the hardware level, where it is first processed. The National Institute of Standards & Technology (NIST) is examining approaches for “lightweight” encryption at the sensor level, which would secure the data as it is collected and pre-processed, without overtaxing MEMS/sensors’ capacity.Chavez also noted that encryption at the edge is increasingly driving security for the IoT and industrial markets, offering tire pressure monitor sensors (TPMS) as a good example. “TPMS devices live autonomously inside the wheel/tire assembly and therefore only share data wirelessly to the network,” he said. “Encryption at the edge is essential to ensure that valid data is transmitted to the vehicle.” Pros and Cons of OTA  While most of us are familiar with our smartphone operating systems’ annoying-but-necessary Over-the-Air (OTA) updates, many are not aware that designers of IoT, biomedical, transportation and industrial devices also frequently employ OTA updates. As the mere idea of an insecure OTA update for an insulin pump, security system or electrical grid strikes fear into even the bravest heart, I asked the SEMI-MSIG experts what they thought of this cybersecurity technique. Pros and Cons of OTA Chong can see both sides of the issue: “On the positive side, once bugs or vulnerabilities are found, they can be fixed quickly. On the negative side, this is a vector itself for mal-intent.” Chavez identified the network connection as the greatest point of vulnerability. He warns that “the more the system is exposed to external connection, the higher the risk.”Wright added that other protections, like establishing preconditions for accepting an OTA update — such as authentication of the source — could also help to ensure that OTA functionality is only used to secure a system, not compromise it. Let’s Fix It  Whether launched by domestic or international attackers, threats to the cybersecurity of MEMS/sensors-enabled wirelessly connected products and systems are real. Let’s Fix It It is clear that some suppliers have already accepted the challenge — and have the technical expertise to design more secure devices that play such a critical role in smart, wirelessly connected systems of every flavor.-- Maria Vetrano is a consultant with the SEMI-MEMS & Sensors Industry Group-- Maria Vetrano is a consultant with the SEMI-MEMS & Sensors Industry Group"
U.S. Firms Still Dominate Chip Market Share Dylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334834,https://m.eet.com/media/1312272/US_Flagball_800.png,Automotive Designline,06.20.19,"SAN FRANCISCO — U.S.-based chip firms held more than 50% of the global IC market share in 2018, by far the most of any region, according to market research firm IC Insights. U.S.-headquartered fabless chip companies accounted for 68% of the global fabless market last year, while U.S. IDMs held 46% of the global IDM share.RecommendedTI Widens Dominance of Analog IC MarketRecommendedTI Widens Dominance of Analog IC MarketSouth Korean companies — chiefly, memory chip giants Samsung Electronics and SK Hynix — accounted for 27% of the worldwide chip industry market share in 2018, up from 24% in 2017, said the firm.Taiwan-headquartered firms and Europe-headquartered firms each accounted for about 6% of the global IC market share last year. The ranking includes fabless chip companies and IDMs but excludes foundries — an area where Taiwan dominates.South Korean and Japanese companies each have an extremely weak presence in the fabless IC segment, and the Taiwanese and Chinese companies have a very low share of the IDM portion of the IC market, IC Insights said. Overall, U.S.-headquartered companies show the most balance with regard to IDM, fabless, and total IC industry market share.Thanks to dramatic increases in average selling prices for DRAM and NAND flash memory last year, South Korean-based chip firms cumulatively increased their sales by 26% last year. As a group, South Korean firms and Chinese chip firms grew faster than the 14% rise in global IC sales last year.But IC Insights noted that with the memory chip market projected to decline by 30% this year, it is likely that South Korean-headquartered companies will rank behind every other region of the world in sales growth in 2019. "
Imec Doubles Energy Density of its Solid-State BatteriesNitin Dahad,https://www.eetimes.com/document.asp?doc_id=1334836,https://m.eet.com/media/1312276/Imec-SST-Batteries-Samples.jpg,Automotive Designline,06.20.19,"Research center Imec has said it has doubled the energy density of its solid-state Li-metal battery cell and started scaling up a pilot line for manufacturing the cells, paving the way for longer range electric vehicles.Making the announcement at the European Electric Vehicle Batteries Summit in Berlin this week, Imec said its battery cell has achieved an energy density of 400 Wh/liter at a charging speed of 0.5C (2 hours). The organization's engineering roadmap for solid-state batteries says it will surpass wet Li-ion battery performance and reach 1000Wh/L at 2-3C by 2024.Today’s rechargeable Li-ion battery technology still has room for improvement, but not enough to significantly improve the range and autonomy of electrical vehicles. Hence, Imec’s researchers are working to replace the wet electrolyte with a solid material, which provides a platform to further increase the energy density of the cell beyond that of cells based on liquid electrolyte.The solid nanocomposite electrolyte that the R&D center has developed has an exceptionally high conductivity of up to 10 mS/cm with a potential for even higher conductivities. A distinguishing feature of the new material is that it is applied as a liquid — via wet chemical coating – and only afterwards converted into a solid when it is already in place in the electrodes. That way it is perfectly suited to be casted into dense powder electrodes where it fills all cavities and makes maximum contact, just as a liquid electrolyte does.Using that solid nanocomposite electrolyte in combination with a standard lithium iron phosphate (LFP) cathode and lithium metal anode, Imec says it has now fabricated an improved battery with a record combination of energy density and charging speed for a solid-state battery.Imec’s solid state battery cells [Image: Imec]Imec’s solid state battery cells [Image: Imec]In addition, Imec has commenced the upscaling of the cells in a state-of-the-art lab for this new solid-state battery technology, including a 300 square meter battery assembly pilot line which includes a dry room of 100 square meters. This conventional A4 sheet-to-sheet wet coating-based line is suited to processing of Imec’s solid electrolyte.The assembly of the new cells could be done by slight modification of existing manufacturing lines for Li-ion batteries. This means the new technology would not need expensive capital investments to switch from wet to solid-state cells. The new pilot line is located at its partner the EnergyVille Campus in Genk (Belgium); another is set-up with the university of Hasselt. These lines can produce prototype pouch cells of up to 5Ah capacity. The EnergyVille line is ready to become a cornerstone for research groups and companies doing R&D projects on these batteries.“The new battery demonstrates that our breakthrough electrolyte can be integrated in performant batteries. The pilot-line allows us to take the next step and upscale the battery breakthrough to industrially relevant processes and formats, using manufacturing processes similar to those for wet batteries,” said Philippe Vereecken, scientific director at Imec/EnergyVille.Imec’s battery R&D is a collaborative program for open innovation to which it says it invites all interested parties."
"TSMC, Purdue Team Up to Enhance Chip SecurityAlan Patterson",https://www.eetimes.com/document.asp?doc_id=1334823,https://m.eet.com/content/images/edn/TSMC%20logo_cut%20off_366_1490162663.png,Industrial Control Designline,06.18.19,"INDIANAPOLIS – Taiwan’s Taiwan Semiconductor Manufacturing Co. (TSMC) and Purdue University in the U.S. announced the establishment of a center at the university to enhance semiconductor security, via a press statement last week. The Center for Secured Microelectronics Ecosystem is aimed at safeguarding the semiconductor and tool supply chain from the foundry stage to packaging.The world’s largest foundry and Purdue officials announced the agreement in Washington, D.C. during the SelectUSA Conference held in mid-June. The annual conference, led by the Department of Commerce, combines a number of U.S. government agencies to facilitate investment in the U.S. The move comes as chip-level security raises new worries, not the least of which involves cyberespionage. The advent of AI opens a wide range of issues highlighted in research in areas such as poisoning of training sets and adversarial input. Outside the AI realm, as the number of edge devices surges, vulnerabilities such as Meltdown and Spectre have raised concerns about data leakage, data theft, and ransomware.“Semiconductors will continue to be the enabling backbone for technological and economic growth in the 21st century, propelling advancements in IoT, autonomous transportation, AI, advanced manufacturing, and many other applications,” said U.S. Secretary of Commerce Wilbur Ross. “Under President Trump’s leadership, the administration will continue doing all it can to grow and equip our highly skilled workforce, maintain our competitive investment, and regulatory environment and support world-class American research universities.”The partnership may draw another line between the U.S. and China in the competition between the two nations to dominate new technologies such as 5G and AI in the future. The Trump Administration has banned suppliers of U.S. semiconductor technology from selling to China’s Huawei, the world’s biggest telecom equipment company, without special approval, because of what the government said were national security issues. TSMC headquarters. (Source: TSMC) TSMC headquarters. (Source: TSMC)Secure Ecosystem  The center will be located at the Purdue campus in West Lafayette, Indiana, to conduct research on building a secure ecosystem for the manufacture of microelectronics systems. The Purdue School of Electrical and Computer Engineering, which will lead the R&D effort, has about 10 faculty members already collaborating with TSMC.Secure Ecosystem “TSMC is pleased to have this opportunity to support Purdue’s world-class technology research,” said H.S. Philip Wong, TSMC’s vice president of corporate research. “TSMC believes it would be beneficial to contribute to the development of a secure electronics ecosystem.”TSMC’s commitment is only the beginning of what will be an internationally significant initiative, Purdue University President Mitch Daniels said.“Purdue and TSMC will engage with additional universities in the U.S. and possibly other companies in the ecosystem,” Purdue University College of Engineering Dean Mung Chiang told EE Times in emailed comments. He declined to provide details on specific areas of research.EE Times“This industry is one of the most important for our global economy and security,” said Chad Pittman, vice president of the Purdue Research Foundation National Security and Defense Program Office and Government Relations. “The strengths in research and development of Purdue and TSMC will help support and advance this critical industry on multiple levels.”   The agreement also allows TSMC to help facilitate access to multi-project wafer shuttle runs to test the effectiveness of the proposed research and to assign representatives on the advisory board of the center to mentor specific projects. "
Chiplet Ecosystem Slowly Picks up SteamDylan McGrath,https://www.eetimes.com/document.asp?doc_id=1334826,https://m.eet.com/media/1312222/190618_Ramune_800_2-min.png,SoC Designline,06.18.19,"SANTA CLARA, Calif. — Momentum continues to coalesce slowly around the creation of an open-chiplet ecosystem, enabling the heterogeneous integration of chiplets from multiple vendors in a system-in-package (SiP).Chiplets represent one of several efforts to compensate for slowing performance gains through brute-force scaling; it’s the slowing of Moore’s Law. While individual chip companies, including Intel, Marvell, and startup zGlue — as well as system companies such as Cisco — have had some success in creating their own chiplet ecosystems, efforts to date have relied on proprietary multi-chip interfaces.RecommendedChiplet Effort Plays First ProposalsRecommendedChiplet Effort Plays First ProposalsThe development of an industry-wide open-chiplet ecosystem that would allow designers to assemble “best of breed” chips incorporating components from multiple vendors requires not only standard open interfaces but also technology advancements in areas such as wafer testing and thermal management and the creation of new business models.Steady — if slow — progress around an open-chiplet ecosystem was on display last week at the second workshop of the Open Domain-Specific Architecture (ODSA) group, which now claims some 70 member companies and is working to define an open interface for chiplet-based design and seeks the creation of a stack-compliant interoperable chiplet marketplace. ODSA operates under the umbrella of the Open Compute Project.According to Ramune Nagisetty, a senior principal engineer at Intel and director of process and product integration in Intel’s CTO office, the need for chiplets is based on the emergence of new workloads combined with enabling advancements in architectures and packaging technologies.“The industry has reached an inflection point,” said Nagisetty. “We have the opportunity to continue scaling package integration through the innovation of chiplets.”A year ago, Intel released its AIB protocol for its EMIB package as part of its work in a DARPA research program on chiplets. At the ODSA workshop last week, Intel followed that up by releasing version 5.2 of PHY Interface for PCI Express (PIPE), a stripped-down version of the PCI Express interface that is described as a configurable short-reach PHY.Meanwhile, the ODSA group continues to work on advancing its own open bunch-of-wires (BOW) physical layer interface. At the ODSA workshop, Bapi Vinnakota, an engineer at Netronome and the leader of the ODSA working group, actively recruited foundry and chiplet support for the BOW interface and introduced a new chiplet design exchange project to enable firms to make open-chiplet physical descriptions that are normally kept confidential in machine-readable form using a data-exchange format developed by zGlue.“It’s very difficult to share confidential information when you are working in an open organization,” Vinnakota said.The BOW interface was originally proposed during ODSA’s first workshop in March. The idea of BOW is to provide a common and simple parallel interface that operates at a relatively low data rate to enable chips in older nodes to be integrated into the system, such as integrating critical RF components-advanced SiPs.NEXT PAGE: Growing painsNEXT PAGE: Growing pains"
Let’s Cut the Hype About RobocarsJunko Yoshida,https://www.eetimes.com/document.asp?doc_id=1334828,https://m.eet.com/media/1312245/NXP_selfdrivingcartestingplatform-min.jpg,Automotive Designline,06.18.19,"It’s time to have an honest discussion about self-driving cars. Nobody is likely to start getting a return on their investment in autonomous vehicle (AV) technology before 2025, and a fully self-driving vehicle is unlikely before 2030. Those are both optimistic estimates.And those estimates are coming from companies in the automotive industry. A panel held at NXP’s event last week elicited some surprisingly candid views on the state of (AV) technology since an Uber fatality and several Tesla accidents.One panel — “Self-Driving Cars: What’s the Payoff for Carmakers?” — featured two representatives from tech suppliers and two EV OEM executives. They covered ground ranging from the “safety versus cost” debate to “how safe AVs must be” and “how soon” before the public will trust an AV: Decades? Or sooner?The panel’s assessment had an unusually high level of realism. Still fuzzy, however, were specifics on how tech companies and carmakers plan to recover from the billions of dollars they have poured into the development of AV technologies. Will they ever see a payoff?The following are my nine takeaways after listening to their panels:1. Why do we need highly automated vehicles? We’ve been talked into AVs because, presumably, machine-driven cars can make the roads safer.1.Why do we need highly automated vehicles?Officially, this is all about safety.According to the World Health Organization (WHO), nearly 1.35 million people die in road crashes each year. Road-traffic injuries are now the leading killer of people aged 5–29 years.Both automotive and tech industries agree: Reducing these numbers is the No. 1 goal.Furthermore, by taking unreliable human drivers out of the equation, they promise that AVs will not only keep the streets safe but will enhance mobility for people unable to drive — children, the handicapped, and senior citizens. Robotaxi fleets will cut the cost of cab transportation and truck platooning will reduce air pollution.In sum, the AV industry is pushing all of the right buttons in hopes of gaining the trust and support of governments and the public. It’s hard to argue against safety.But this is 2019. The industry has been working on autonomous vehicles, at a staggering cost, for years. Has capitalism suddenly decided to put human safety ahead of profit?2. This is about the automotive industry’s survival.2.This is about the automotive industry’s survival.Tech companies and automakers are jumping through hoops to develop highly automated technologies. Why?Safety is a MacGuffin — a plot device. Its importance to the plot is not in safety for safety’s sake but in its impact on the car market and carmakers’ motives.Last week, listening to the NXP forum, it was almost a relief when panelist Wesley Shao, director of Autonomous Driving at EV startup Byton, bluntly stated: “This isn’t about cost. It’s not about safety. This is about carmakers’ survival.”This sort of honesty pervaded the panel.Alexander Tan, NXP’s vice president and general manager of Automotive Ethernet Solutions, shared two scenarios of the future AV industry. One is the rise of mobility services with self-driving vehicle technology as the norm — particularly in urban settings. The other scenario is widespread self-driving car ownership.Two competing factors, in either scenario, are “safety and cost,” Tan said.If self-driving cars prove to be extremely safe, people will more swiftly adopt AVs, and regulations will be supportive. But right now, it’s not possible for the normal architecture used in consumer-owned cars to compete in cost with a car designed for mobility as a service.“Unless car OEMs have the innovation and the vision to change the architecture to support these features at the right cost,” Tan predicted, carmakers will remain in the older model and find themselves outcompeted in this AV space.Now, if that’s not a scary scenario for car OEMs, what is?Next page: AVs are still decades away from driving a vehicle at the same level as humans Next page: AVs are still decades away from driving a vehicle at the same level as humans "
PCIe Preps for 64G LeapRick Merritt,https://www.eetimes.com/document.asp?doc_id=1334829,https://m.eet.com/media/1312249/PCISIGlogo-min.png,Wireless and Networking Designline,06.18.19,"SANTA CLARA, Calif. — PCI Express (PCIe) will get a 6.0 spec in 2021, enabling data rates up to 64 gigatransfers per second (GT/s) and leveraging PAM-4 modulation. The news shows that copper interconnects will have a long life, albeit with an increasingly short reach.The PCI Special Interest Group (SIG) is bringing to mainstream designers the PAM-4 capabilities that SerDes developers are already running at 56G and beyond for high-end systems. At the bleeding edge, other groups already have multiple 112G specs in the works, and some experts say there’s a clear line of sight to 200G copper links and beyond.The perpetual tradeoff is that the faster the link is, the shorter the distance that it can travel. There are some caveats. The tradeoff can be mitigated by adopting more expensive printed circuit board materials or retimer chips. Another consideration is that PAM-4 requires forward error correction (FEC) blocks that add latency.Systems designers are already moving to cabled links inside servers and networking gear to avoid the costs of retimers and premium board materials. The SIG is still debating what latency its 6.0 spec will support, but one expert said that it will need to match the latency of DRAMs measured in tens of nanoseconds.PAM-4 and FEC are new to the PCIe specs, which up to now have relied on a more relaxed non-return-to-zero (NRZ) technique.“It will be challenging,” said Al Yanes, president of the SIG. “We’re going to squeeze everywhere — materials, connectors — nothing is free … It’s all about the PHY and analog and bit error rates, but we’re lucky our organization has a lot of smart engineers.”Gen6, as the 6.0 spec is also known, will need to be backward-compatible with all earlier PCIe specs so that motherboards and adapter cards can evolve at different timeframes. Delivering a spec that lets products shift between NRZ and PAM-4 schemes presents “a tax burden,” Yanes said.Large cloud computing providers are among the drivers for the faster speeds. The SIG finished a 32-GT/s Gen5 spec just last month that is already getting tapeouts in chips for AI accelerators, data center processors, and storage systems. The move to 400G and 800G networks at big data centers is also driving the need for fast interconnects.“We rested for a month before we called a face-to-face with our electrical working group [on Gen6] because it’s all about the PHY these days,” Yanes said, referring to the highly analog-based physical layer block.Next page: Dealing with shorter reaches and rising costsNext page: Dealing with shorter reaches and rising costs"
